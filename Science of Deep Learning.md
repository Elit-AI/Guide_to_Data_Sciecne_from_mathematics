# Science of Deep Learning

[Deep learning is a transformative technology that has delivered impressive improvements in image classification and speech recognition. Many researchers are trying to better understand how to improve prediction performance and also how to improve training methods. Some researchers use experimental techniques; others use theoretical approaches.](https://stats385.github.io/)

* [Explainable AI: Interpreting, Explaining and Visualizing Deep Learning](https://link.springer.com/book/10.1007/978-3-030-28954-6)
* [Analyses of Deep Learning (STATS 385) 2019](https://stats385.github.io/)
* [The Science of Deep Learning](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_5058.html)
* [torbenkruegermath](https://sites.google.com/site/torbenkruegermath/home/graduate-seminar-random-matrices-spin-glasses-deep-learning)
* [6.883 Science of Deep Learning: Bridging Theory and Practice -- Spring 2018](https://people.csail.mit.edu/madry/6.883/)
* [(Winter 2018) IFT 6085: Theoretical principles for deep learning](http://mitliagkas.github.io/ift6085-dl-theory-class/)
* http://principlesofdeeplearning.com/
* https://cbmm.mit.edu/education/courses
* [DALI 2018 - Data, Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
* [On Theory@http://www.deeplearningpatterns.com ](http://www.deeplearningpatterns.com/doku.php?id=theory)
* https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/85815724
* [UVA DEEP LEARNING COURSE](https://uvadlc.github.io/)
* [Understanding Neural Networks by embedding hidden representations](https://rakeshchada.github.io/Neural-Embedding-Animation.html)
* [Tractable Deep Learning](https://www.cs.washington.edu/research/tractable-deep-learning)
* [Theories of Deep Learning (STATS 385)](https://stats385.github.io/)
* [Topics Course on Deep Learning for Spring 2016 by Joan Bruna, UC Berkeley, Statistics Department](https://github.com/joanbruna/stat212b)
* [Mathematical aspects of Deep Learning](http://elmos.scripts.mit.edu/mathofdeeplearning/)
* [MATH 6380p. Advanced Topics in Deep Learning Fall 2018](https://deeplearning-math.github.io/)
* [CoMS E6998 003: Advanced Topics in Deep Learning](https://www.advancedtopicsindeeplearning.com/)
* [Deep Learning Theory: Approximation, Optimization, Generalization](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html)
* [Theory of Deep Learning, ICML'2018](https://sites.google.com/site/deeplearningtheory/)
* [DALI 2018, Data Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
* [MATHEMATICS OF DEEP LEARNING, NYU, Spring 2018](https://github.com/joanbruna/MathsDL-spring18)
* [Theory of Deep Learning, project in researchgate](https://www.researchgate.net/project/Theory-of-Deep-Learning)
* [THE THEORY OF DEEP LEARNING - PART I](https://physicsml.github.io/blog/DL-theory.html)
* [Magic paper](http://cognitivemedium.com/magic_paper/index.html)
* [Principled Approaches to Deep Learning](https://www.padl.ws/)
* [A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/pdf/1811.03962.pdf)
* [Advancing AI through cognitive science](https://github.com/brendenlake/AAI-site)
* [Deep Learning and the Demand for Interpretability](http://stillbreeze.github.io/Deep-Learning-and-the-Demand-For-Interpretability/)
* https://beenkim.github.io/
* [Integrated and detailed image understanding](https://www.robots.ox.ac.uk/~vedaldi//research/idiu/idiu.html)
* [NeuroIP 2018 workshop on Deep Learning Theory](http://nips2018dltheory.rice.edu/)
* http://networkinterpretability.org/
* https://interpretablevision.github.io/
* https://www.msra.cn/zh-cn/news/people-stories/wei-chen
* https://www.microsoft.com/en-us/research/people/tyliu/
* https://zhuanlan.zhihu.com/p/22353056
* http://qszhang.com/index.php/team/
* https://www.researchgate.net/profile/Hatef_Monajemi

### Deep Learning Reading Group

[yanjun](http://www.cs.virginia.edu//papers.htm) organized a wonderful reading group on deep learning.

- https://a2i2.deakin.edu.au/
- https://qdata.github.io/deep2Read/
- https://dlta-reading.github.io/
* http://www.mlnl.cs.ucl.ac.uk/readingroup.html
* https://labrosa.ee.columbia.edu/cuneuralnet/
* http://www.ub.edu/cvub/reading-group/
* https://team.inria.fr/perception/deeplearning/
* https://scholar.princeton.edu/csmlreading
* https://junjuew.github.io/elijah-reading-group/
* http://www.sribd.cn/DL/schedule.html
* http://lear.inrialpes.fr/people/gaidon/lear_xrce_deep_learning_01.html
* https://simons.berkeley.edu/events/reading-group-deep-learning
* https://csml.princeton.edu/readinggroup
* http://www.bicv.org/deep-learning/
* https://www.cs.ubc.ca/labs/lci/mlrg/
* https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
* https://project.inria.fr/deeplearning/
* https://hustcv.github.io/reading-list.html

## Physics and Deep Learning
[Neuronal networks have enjoyed a resurgence both in the worlds of neuroscience, where they yield mathematical frameworks for thinking about complex neural datasets, and in machine learning, where they achieve state of the art results on a variety of tasks, including machine vision, speech recognition, and language translation.   Despite their empirical success, a mathematical theory of how deep neural circuits, with many layers of cascaded nonlinearities, learn and compute remains elusive.  We will discuss three recent vignettes in which ideas from statistical physics can shed light on this issue.  In particular, we show how dynamical criticality can help in neural learning, how the non-intuitive geometry of high dimensional error landscapes can be exploited to speed up learning, and how modern ideas from non-equilibrium statistical physics, like the Jarzynski equality, can be extended to yield powerful algorithms for modeling complex probability distributions.  Time permitting, we will also discuss the relationship between neural network learning dynamics and the developmental time course of semantic concepts in infants.](https://physics.berkeley.edu/news-events/events/20151005/the-statistical-physics-of-deep-learning-on-the-beneficial-roles-of)

<img src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2017/09/InfoBottleneck_2880x1620.jpg" width="80%"/>

* [Physics Based Vision meets Deep Learning (PBDL)](https://pbdl2019.github.io/)
* [The thermodynamics of learning](https://phys.org/news/2017-02-thermodynamics.html)
* [Physics-Based Deep Learning](https://github.com/thunil/Physics-Based-Deep-Learning)
* https://dl4physicalsciences.github.io/
* https://maziarraissi.github.io/PINNs/
* http://deeplearnphysics.org/
* [Machine Learning for Physics and the Physics of Learning](https://www.ipam.ucla.edu/programs/long-programs/machine-learning-for-physics-and-the-physics-of-learning/)
* https://www.ias.edu/events/deep-learning-physics
* [A Differentiable Physics Engine
for Deep Learning](http://phys.csail.mit.edu/papers/1.pdf)
* https://gogul.dev/software/deep-learning-meets-physics
* [Quantum Deep Learning and Renormalization](http://www.math.chalmers.se/~stig/project4.pdf)
* https://physics-ai.com/
* http://physics.usyd.edu.au/quantum/Coogee2015/Presentations/Svore.pdf
* [Brains, Minds and Machines Summer Course](https://ocw.mit.edu/resources/res-9-003-brains-minds-and-machines-summer-course-summer-2015/index.htm)
* [New Theory Cracks Open the Black Box of Deep Learning](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)
* [Physics-Based Deep Learning for Fluid Flow](http://phys2018.csail.mit.edu/papers/29.pdf)
* [Lecture Note on Deep Learning
and Quantum Many-Body Computation](https://wangleiphy.github.io/lectures/DL.pdf)
* [deep medcine](http://amos3.aapm.org/abstracts/pdf/127-36916-419554-130797.pdf)
* http://www.dam.brown.edu/people/mraissi/publications/
* [Deep Learning in High
Energy Physics](https://dlonsc.github.io/ISC2019/7_Keynote_DL_HEP_SofiaVallecorsa.pdf)
* [MATH + X SYMPOSIUM ON INVERSE PROBLEMS AND DEEP LEARNING IN SPACE EXPLORATION](https://earthscience.rice.edu/mathx2019/)
* [WHY DOES DEEP LEARNING WORK?](https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/)


## Mathematics of Deep Learning

http://rt.dgyblog.com/ref/ref-learning-deep-learning.html

https://github.com/leiwu1990/course.math_theory_nn

http://www.mit.edu/~9.520/fall18/

2018上海交通大学深度学习理论前沿研讨会 - 凌泽南的文章 - 知乎
https://zhuanlan.zhihu.com/p/40097048

https://www.researchgate.net/project/Theories-of-Deep-Learning

[A mathematical theory of deep networks and of why they work as well as they do is now emerging. I will review some recent theoretical results on the approximation power of deep networks including conditions under which they can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. I will also discuss another puzzle around deep networks: what guarantees that they generalize and they do not overfit despite the number of weights being larger than the number of training data and despite the absence of explicit regularization in the optimization?](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html)

Deep Neural Networks and Partial Differential Equations: Approximation Theory and
Structural Properties
Philipp Petersen, University of Oxford

https://memento.epfl.ch/event/a-theoretical-analysis-of-machine-learning-and-par/

- http://at.yorku.ca/c/b/p/g/30.htm
- https://mat.univie.ac.at/~grohs/
- [Topics course Mathematics of Deep Learning, NYU, Spring 18](https://joanbruna.github.io/MathsDL-spring18/)
- https://skymind.ai/ebook/Skymind_The_Math_Behind_Neural_Networks.pdf
- https://github.com/markovmodel/deeptime
- https://omar-florez.github.io/scratch_mlp/
- https://joanbruna.github.io/MathsDL-spring19/
- https://github.com/isikdogan/deep_learning_tutorials
- https://www.brown.edu/research/projects/crunch/machine-learning-x-seminars
- [Deep Learning: Theory & Practice](http://anotherdatum.com/tce_2018.html)
- https://www.math.ias.edu/wtdl
- https://www.ml.tu-berlin.de/menue/mitglieder/klaus-robert_mueller/
- https://www-m15.ma.tum.de/Allgemeines/MathFounNN
- https://www.math.purdue.edu/~buzzard/MA598-Spring2019/index.shtml
- http://mathematics-in-europe.eu/?p=801
- [Discrete Mathematics of Neural Networks: Selected Topics](https://epubs.siam.org/doi/book/10.1137/1.9780898718539?mobileUi=0)
- https://cims.nyu.edu/~bruna/
- https://www.math.ias.edu/wtdl
- https://www.pims.math.ca/scientific-event/190722-pcssdlcm
- [Deep Learning for Image Analysis EMBL COURSE](https://www.embl.de/training/events/2020/MAC20-01/)
- [MATH 6380o. Deep Learning: Towards Deeper Understanding, Spring 2018](https://deeplearning-math.github.io/2018spring.html)
- [Mathematics of Deep Learning, Courant Insititute, Spring 19](https://github.com/joanbruna/MathsDL-spring19)
- http://voigtlaender.xyz/
- http://www.mit.edu/~9.520/fall19/
- [The Mathematics of Deep Learning and Data Science - Programme](https://gateway.newton.ac.uk/event/ofbw46/programme)
+ [Home of Math + Machine Learning + X](https://www.brown.edu/research/projects/crunch/)
+ [Mathematical and Computational Aspects of Machine Learning](http://crm.sns.it/event/451/)
+ [Mathematical Theory for Deep Neural Networks](https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks)
+ [Theory of Deep Learning](https://www.researchgate.net/project/Theory-of-Deep-Learning)
+ [DALI 2018 - Data, Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
+ [BMS Summer School 2019: Mathematics of Deep Learning](https://www.math-berlin.de/academics/summer-schools/2019)
+ [SIAM Conference on Mathematics of Data Science (MDS20)](https://www.siam.org/conferences/cm/conference/mds20)

* [BRIDGING GAME THEORY AND DEEP LEARNING](https://sgo-workshop.github.io/)


## Numerical Analysis for Deep Learning

This section is on insight from numerical analysis to inspire more effective deep learning architecture.

[Many effective networks can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures.](https://web.stanford.edu/~yplu/proj/lm/)

 [We show that residual neural networks can be interpreted as discretizations of a nonlinear time-dependent ordinary differential equation that depends on unknown parameters, i.e., the network weights. We show how this insight has been used, e.g., to study the stability of neural networks, design new architectures, or use established methods from optimal control methods for training ResNets. Finally, we discuss open questions and opportunities for mathematical advances in this area.](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)

<img src="http://www.mathcs.emory.edu/~lruthot/img/DeepLearning.png" width="80%" />

- [Continuous Models: Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/3-NumDNNshort-ContinuousModels.pdf)
- https://arxiv.org/pdf/1904.05657.pdf
- [CS 584 / MATH 789R - Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/math789r-sp20.html)
- [MA 721: Topics in Numerical Analysis: Deep Learning](http://www.ms.uky.edu/~qye/MA721/ma721F17.html)
- [Numerical methods for deep learning](https://github.com/IPAIopen/NumDL-CourseNotes)
- [Short Course on Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)
- http://www.mathcs.emory.edu/~lruthot/teaching.html
- https://www.math.ucla.edu/applied/cam
- http://www.mathcs.emory.edu/~lruthot/
- [Automatic Differentiation of Parallelised Convolutional
Neural Networks - Lessons from Adjoint PDE Solvers](https://autodiff-workshop.github.io/slides/Hueckelheim_nips_autodiff_CNN_PDE.pdf)
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs.](https://www.math.tu-berlin.de/fachgebiete_ag_modnumdiff/angewandtefunktionalanalysis/v_menue/mitarbeiter/kutyniok/v_menue/kutyniok_publications/)
- https://raoyongming.github.io/
- [Bridging Deep Architects and Numerical Differential Equations](https://web.stanford.edu/~yplu/proj/lm/)

### Deep Unrolling

Several recent studies build deep structures by unrolling a particular optimization model that involves task information, i.e., `learning to optime`.

- [Proximal Alternating Direction Network: A Globally Converged Deep Unrolling Framework](https://arxiv.org/abs/1711.07653)
- [A Bridging Framework for Model Optimization and Deep Propagation](http://papers.nips.cc/paper/7685-a-bridging-framework-for-model-optimization-and-deep-propagation)
- [On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems](http://dutmedia.org/FIMA/)
- [Deep unrolling](https://zhuanlan.zhihu.com/p/44003318)
- http://dutmedia.org/
- http://dlutir.dlut.edu.cn/Scholar/Detail/6711
- https://dblp.uni-trier.de/pers/hd/l/Liu:Risheng
- https://github.com/dlut-dimt
- https://www.researchgate.net/project/optimization-numerical-computation-optimal-control

## Dynamics and Deep Learning

Dynamics of deep learning is to  consider deep learning as a dynamic system. For example, the forward feedback network is expressed in the recurrent form:
$$x^{t+1} = f_t(x^{t}),t\in [0, T]$$
where $f_t$ is some proper function called as activation function and $t$ is discrete.

- https://elsc.huji.ac.il/all-publications/1050
- [BRIDGING DEEP NEURAL NETWORKS AND DIFFERENTIAL EQUATIONS FOR IMAGE ANALYSIS AND BEYOND](http://helper.ipam.ucla.edu/publications/glws3/glws3_15460.pdf)
- [A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1751636)
- [NEURAL NETWORKS AS ORDINARY DIFFERENTIAL EQUATIONS](https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/)
- [Dynamical aspects of Deep Learning](https://zhenyu-liao.github.io/pdf/pre/GDD_iCODE.pdf)
- [Dynamical Systems and Deep Learning](http://www.doc.ic.ac.uk/~ae/teaching.html#complex)
- [Dynamic System and Optimal Control Perspective of Deep Learning](https://web.stanford.edu/~yplu/DynamicOCNN.pdf)
- https://zhuanlan.zhihu.com/p/71747175
- https://web.stanford.edu/~yplu/
- https://web.stanford.edu/~yplu/project.html
- [Deep learning for universal linear embeddings of nonlinear dynamics](https://doaj.org/article/9d9172e9bf324cc6ac6d48ff8e234a85)
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](http://ganguli-gang.stanford.edu/pdf/DynamLearn.pdf)

### Neural Ordinary Differential Equations

`Neural ODE`

- [Neural Ordinary Differential Equations](http://www.cs.toronto.edu/~rtqichen/pdfs/neural_ode_slides.pdf)

<img src="https://rkevingibson.github.io/img/ode_networks_1.png" width="80%" />

+ [NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data](https://www.arxiv-vanity.com/papers/1908.03190/)
+ [Neural Ordinary Differential Equations and Adversarial Attacks](https://rajatvd.github.io/Neural-ODE-Adversarial/)
+ [Neural Dynamics and Computation Lab](http://ganguli-gang.stanford.edu/)

## Differential Equation and Deep Learning

This section is on how to use deep learning or more general machine learning to solve  differential equation numerically.

[We derive upper bounds on the complexity of ReLU neural networks approximating the solution maps of parametric partial differential equations. In particular, without any knowledge of its concrete shape, we use the inherent low-dimensionality of the solution manifold to obtain approximation rates which are significantly superior to those provided by classical approximation results. We use this low dimensionality to guarantee the existence of a reduced basis. Then, for a large variety of parametric partial differential equations, we construct neural networks that yield approximations of the parametric maps not suffering from a curse of dimension and essentially only depending on the size of the reduced basis.](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)

- https://arxiv.org/abs/1804.04272
- https://deepai.org/machine-learning/researcher/weinan-e
- https://deepxde.readthedocs.io/en/latest/
- https://github.com/IBM/pde-deep-learning
- https://github.com/ZichaoLong/PDE-Net
- https://github.com/amkatrutsa/DeepPDE
- https://github.com/maziarraissi/DeepHPMs
- https://github.com/markovmodel/deeptime
- [SPNets: Differentiable Fluid Dynamics for Deep Neural Networks](https://rse-lab.cs.washington.edu/papers/spnets2018.pdf)
- https://maziarraissi.github.io/DeepHPMs/
- [DGM: A deep learning algorithm for solving partial differential equations](https://www.sciencedirect.com/science/article/pii/S0021999118305527)
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)
- [NeuralNetDiffEq.jl: A Neural Network solver for ODEs](https://julialang.org/blog/2017/10/gsoc-NeuralNetDiffEq)
- [PIMS CRG Summer School: Deep Learning for Computational Mathematics](https://www.pims.math.ca/scientific-event/190722-pcssdlcm)
- [Deep Approximation via Deep Learning](http://ins.sjtu.edu.cn:3300/conferences/7/talks/314)
* https://arxiv.org/abs/1806.07366
* https://mat.univie.ac.at/~grohs/
* https://rse-lab.cs.washington.edu/



## Approximation Theory for Deep Learning

Universal approximation theory show the expression power of deep neural network of some wide while shallow neural network.
The section will extend the approximation to the deep neural network.

[We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in $L^2(\mathbb R^d)$. In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy.](https://epubs.siam.org/doi/pdf/10.1137/18M118709X)

- [Approximation Analysis of Convolutional Neural Networks](https://cpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/d/11132/files/2019/07/paper_cnn_copy.pdf)
- [Deep vs. shallow networks : An approximation theory perspective](https://arxiv.org/abs/1608.03287)
- [Deep Neural Network Approximation Theory](https://arxiv.org/abs/1901.02220)
- [Provable approximation properties for deep neural networks](https://cpsc.yale.edu/sites/default/files/files/tr1513(1).pdf)
- [Optimal Approximation with Sparsely Connected Deep Neural Networks](https://epubs.siam.org/doi/pdf/10.1137/18M118709X)
- [Deep Learning: Approximation of Functions by Composition](http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf)
- [DGD Approximation Theory Workshop](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Petersen/DGD_Approximation_Theory.pdf)
- [Deep Neural Networks: Approximation Theory and Compositionality](http://www.mit.edu/~9.520/fall16/Classes/deep_approx.html)
- [DNN Bonn](http://voigtlaender.xyz/DNNBonnHandout.pdf)
- [From approximation theory to machine learning](http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf)
- [Collapse of Deep and Narrow Neural Nets](https://arxiv.org/abs/1808.04947)
- [Nonlinear Approximation and (Deep) ReLU Networks](https://www.math.tamu.edu/~foucart/publi/DDFHP.pdf)
- [Deep Approximation via Deep Learning](http://www.ipam.ucla.edu/abstract/?tid=15953&pcode=GLWS3)
- [Convolutional Neural Networks for Steady Flow Approximation](https://github.com/loliverhennigh/Steady-State-Flow-With-Neural-Nets)
- https://www.cityu.edu.hk/ma/people/profile/zhoudx.htm

### The F-Principle

[We aim to develop a theoretical framework on Fourier domain to analyze the Deep Neural Network (DNN) training process and understand the DNN generalization. We exemplified our theoretical results through DNNs fitting 1-d functions and the MNIST dataset.](https://www.researchgate.net/project/Deep-learning-in-Fourier-domain)

- [Deep learning in Fourier domain](https://www.researchgate.net/project/Deep-learning-in-Fourier-domain)
- [Deep Learning Theory: The F-Principle and An Optimization Framework](http://ins.sjtu.edu.cn:3300/conferences/7/talks/319)
- [Frequency Principle in Deep Learning with General Loss Functions and Its Potential Application](https://arxiv.org/abs/1811.10146)
- [Theory of the Frequency Principle for General Deep Neural Networks](https://arxiv.org/pdf/1906.09235v1.pdf)
- https://arxiv.org/pdf/1905.10264.pdf
- https://www.researchgate.net/profile/Zhiqin_Xu

## Inverse Problem and Deep Learning

[There is a long history of algorithmic development for solving inverse problems arising in sensing and imaging systems and beyond. Examples include medical and computational imaging, compressive sensing, as well as community detection in networks. Until recently, most algorithms for solving inverse problems in the imaging and network sciences were based on static signal models derived from physics or intuition, such as wavelets or sparse representations.](https://deep-inverse.org/)

[Today, the best performing approaches for the aforementioned image reconstruction and sensing problems are based on deep learning, which learn various elements of the method including i) signal representations, ii) stepsizes and parameters of iterative algorithms, iii) regularizers, and iv) entire inverse functions. For example, it has recently been shown that solving a variety of inverse problems by transforming an iterative, physics-based algorithm into a deep network whose parameters can be learned from training data, offers faster convergence and/or a better quality solution. Moreover, even with very little or no learning, deep neural networks enable superior performance for classical linear inverse problems such as denoising and compressive sensing. Motivated by those success stories, researchers are redesigning traditional imaging and sensing systems.](https://deep-inverse.org/)

- [Sixteenth International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research](http://cpaior2019.uowm.gr/)
- [Neumann Networks for Inverse Problems in Imaging](https://arxiv.org/abs/1901.03707)
- https://github.com/mughanibu/Deep-Learning-for-Inverse-Problems
- [Accurate Image Super-Resolution Using Very Deep Convolutional Networks](https://cv.snu.ac.kr/research/VDSR/)
- [Deep Learning for Inverse Problems](https://arxiv.org/abs/1803.00092)
- [Solving inverse problems with deep networks](https://deep-inverse.org/)
- https://earthscience.rice.edu/mathx2019/
- [Deep Learning and Inverse Problem](https://www.dlip.org/)
- https://www.scec.org/publication/8768
- [deep inverse optimization](https://github.com/tankconcordia/deep_inv_opt)
- https://amds123.github.io/

## Random Matrix Theory and Deep Learning

Random matrix focus on the matrix, whose entities are sampled from  some specific probability distribution.
Weight matrices in deep nerual network are initialed in random.
However, the model is over-parametered and it is hard to verify the role of one individual parameter.

- https://project.inria.fr/paiss/
- https://zhenyu-liao.github.io/activities/
- [Recent Advances in Random Matrix Theory for Modern Machine Learning](https://zhenyu-liao.github.io/pdf/pre/Matrix_talk_liao_handout.pdf)
- http://romaincouillet.hebfree.org/
- https://zhenyu-liao.github.io/
- https://dionisos.wp.imt.fr/
- [Features extraction using random matrix theory](https://ir.library.louisville.edu/cgi/viewcontent.cgi?article=2227&context=etd)
- [Nonlinear random matrix theory for deep learning](https://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [A RANDOM MATRIX APPROACH TO NEURAL NETWORKS](https://arxiv.org/pdf/1702.05419.pdf)
- [A Random Matrix Approach to Echo-State Neural Networks](http://proceedings.mlr.press/v48/couillet16.pdf)
- [Harnessing neural networks: A random matrix approach](https://hal.archives-ouvertes.fr/hal-01962073)
- [Tensor Programs: A Swiss-Army Knife for Nonlinear Random Matrix Theory of Deep Learning and Beyond](https://www.csail.mit.edu/event/tensor-programs-swiss-army-knife-nonlinear-random-matrix-theory-deep-learning-and-beyond)
- [Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation](https://arxiv.org/abs/1902.04760)
- https://romaincouillet.hebfree.org/docs/conf/ELM_icassp.pdf
- https://romaincouillet.hebfree.org/docs/conf/NN_ICML.pdf
- http://www.vision.jhu.edu/tutorials/CVPR16-Tutorial-Math-Deep-Learning-Raja.pdf

## Deep learning and Optimal Transport

[Optimal transport (OT) provides a powerful and flexible way to compare probability measures, of all shapes: absolutely continuous, degenerate, or discrete. This includes of course point clouds, histograms of features, and more generally datasets, parametric densities or generative models. Originally proposed by Monge in the eighteenth century, this theory later led to Nobel Prizes for Koopmans and Kantorovich as well as Villani’s Fields Medal in 2010.](http://otml17.marcocuturi.net/)

- [Optimal Transport & Machine Learning](http://otml17.marcocuturi.net/)
- [Topics on Optimal Transport in Machine Learning and Shape Analysis (OT.ML.SA)](https://people.math.osu.edu/memolitechera.1/courses/cse-topics-2018/)
- https://www-obelix.irisa.fr/files/2017/01/postdoc-Obelix.pdf
- http://www.cis.jhu.edu/~rvidal/talks/learning/StructuredFactorizations.pdf
- http://cmsa.fas.harvard.edu/wp-content/uploads/2018/06/David_Gu_Harvard.pdf
- https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/
- https://www.louisbachelier.org/wp-content/uploads/2017/07/170620-ilb-presentation-gabriel-peyre.pdf
- http://people.csail.mit.edu/davidam/
- https://www.birs.ca/events/2020/5-day-workshops/20w5126
- https://github.com/hindupuravinash/nips2017
- [Selection dynamics for deep neural networks](https://arxiv.org/abs/1905.09076v1)
- https://people.math.osu.edu/memolitechera.1/index.html

## Geometric Analysis Approach to AI

[Why and how that deep learning works well on different tasks remains a mystery from a theoretical perspective. In this paper we draw a geometric picture of the deep learning system by finding its analogies with two existing geometric structures, the geometry of quantum computations and the geometry of the diffeomorphic template matching. In this framework, we give the geometric structures of different deep learning systems including convolutional neural networks, residual networks, recursive neural networks, recurrent neural networks and the equilibrium prapagation framework. We can also analysis the relationship between the geometrical structures and their performance of different networks in an algorithmic level so that the geometric framework may guide the design of the structures and algorithms of deep learning systems.](https://arxiv.org/pdf/1710.10784.pdf)

- [ABC Dataset A Big CAD Model Dataset For Geometric Deep Learning](https://deep-geometry.github.io/abc-dataset/)
- [How deep learning works — The geometry of deep learning](https://arxiv.org/pdf/1710.10784.pdf)
- http://cmsa.fas.harvard.edu/geometric-analysis-ai/
- http://inspirehep.net/record/1697651
- https://diglib.eg.org/handle/10.2312/2631996
- http://ubee.enseeiht.fr/skelneton/
- https://biomedicalimaging.org/2019/tutorials/
- [Geometric View to Deep Learning](http://valser.org/article-269-1.html)
- [GEOMETRIC IDEAS IN MACHINE LEARNING: FROM DEEP LEARNING TO INCREMENTAL OPTIMIZATION](https://www.isi.edu/events/calendar/12459/)
- [Deep Learning Theory: Geometric Analysis of Capacity, Optimization, and Generalization for Improving Learning in Deep Neural Networks](https://cordis.europa.eu/project/rcn/214602/factsheet/en)
- [Workshop IV: Deep Geometric Learning of Big Data and Applications](http://www.ipam.ucla.edu/programs/workshops/workshop-iv-deep-geometric-learning-of-big-data-and-applications/)
- [Robustness and geometry of deep neural networks](https://gateway.newton.ac.uk/sites/default/files/asset/doc/1905/Alhussein_Fawzi.pdf)
- [A geometric view of optimal transportation and generative model](https://www.sciencedirect.com/science/article/pii/S0167839618301249)
- [Optimal Transport Theory the New Math for Deep Learning](https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/)
- [GeoNet: Deep Geodesic Networks for Point Cloud Analysis](http://openaccess.thecvf.com/content_CVPR_2019/papers/He_GeoNet_Deep_Geodesic_Networks_for_Point_Cloud_Analysis_CVPR_2019_paper.pdf)
- https://www.nsf.gov/awardsearch/showAward?AWD_ID=1418255
- https://nsf-tripods.org/institutes/
- https://users.math.msu.edu/users/wei/
- https://www.darpa.mil/program/hierarchical-identify-verify-exploit

## Topology and Deep Learning

[We perform topological data analysis on the internal states of convolutional deep neural networks to develop an understanding of the computations that they perform. We apply this understanding to modify the computations so as to (a) speed up computations and (b) improve generalization from one data set of digits to another. One byproduct of the analysis is the production of a geometry on new sets of features on data sets of images, and use this observation to develop a methodology for constructing analogues of CNN's for many other geometries, including the graph structures constructed by topological data analysis.](https://arxiv.org/abs/1811.01122)

- [Topological Methods for Machine Learning](http://topology.cs.wisc.edu/)
- [A Topology Layer for Machine Learning](http://ai.stanford.edu/blog/topologylayer/)
- [Topological Approaches to Deep Learning](https://arxiv.org/abs/1811.01122)
- https://www.gaotingran.com/
- [Topology based deep learning for biomolecular data](https://users.math.msu.edu/users/wei/AIM.pdf)
- [RESEARCH ARTICLE TopologyNet: Topology based deep convolutional and multi-task neural networks for biomolecular property predictions](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005690)
- [Exposition and Interpretation of the Topology of Neural Networks](https://arxiv.org/abs/1810.03234)
- https://zhuanlan.zhihu.com/p/26515275
- [Applying Topological Persistence in Convolutional Neural Network for Music Audio Signals](https://arxiv.org/pdf/1608.07373.pdf)
- [Towards a topological–geometrical theory of group equivariant non-expansive operators for data analysis and machine learning](https://www.nature.com/articles/s42256-019-0087-3)

## Information Theory and Deep Learning

[In short, Neural Networks extract from the data the most relevant part of the information that describes the statistical dependence between the features and the labels. In other words, the size of a Neural Networks specifies a data structure that we can compute and store, and the result of training the network is the best approximation of the statistical relationship between the features and the labels that can be represented by this data structure.](https://lizhongresearch.miraheze.org/wiki/Understanding_the_Power_of_Neural_Networks)

* [Information Theory of Deep Learning](https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/)
* [Anatomize Deep Learning with Information Theory](https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html)
* [“Deep learning - Information theory & Maximum likelihood.”](https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/)
* [On the information bottleneck theory of deep learning](https://www.researchgate.net/publication/325022755_On_the_information_bottleneck_theory_of_deep_learning)
* [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406.pdf)
* [Information Theoretic Interpretation of Deep Neural Networks](https://lids.mit.edu/news-and-events/events/information-theoretic-interpretation-deep-neural-networks)
- https://lizhongresearch.miraheze.org/wiki/Main_Page
- https://lizhongzheng.mit.edu/
- 
- https://www.leiphone.com/news/201703/qzBcOeDYFHtYwgEq.html
- http://nsfcbl.org/
- [Large Margin Deep Neural Networks: Theory and Algorithms](https://arxiv.org/abs/1506.05232v1)
- http://ai.stanford.edu/
