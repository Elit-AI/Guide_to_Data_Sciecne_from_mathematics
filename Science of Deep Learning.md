# Science of Deep Learning

- https://nthu-datalab.github.io/ml/index.html
- http://www.cs.cornell.edu/~shmat/research.html

[Deep learning is a transformative technology that has delivered impressive improvements in image classification and speech recognition. Many researchers are trying to better understand how to improve prediction performance and also how to improve training methods. Some researchers use experimental techniques; others use theoretical approaches.](https://stats385.github.io/)


* [Deep Learning Drizzle](https://deep-learning-drizzle.github.io/)
* [A Comprehensive Analysis of Deep Regression](https://github.com/Stephlat/DeepRegression)
* https://gangwg.github.io/research.html
* http://www.mit.edu/~k2smith/
* [2018 Workshop on Interpretable & Reasonable Deep Learning and its Applications (IReDLiA)](http://web.fsktm.um.edu.my/~cschan/iredlia.html)
* [Analyses of Deep Learning (STATS 385) 2019](https://stats385.github.io/)
* [The Science of Deep Learning](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_5058.html)
* [TBSI 2019 Retreat Conference](http://workshop.tbsi.edu.cn/index.html)
* [6.883 Science of Deep Learning: Bridging Theory and Practice -- Spring 2018](https://people.csail.mit.edu/madry/6.883/)
* [(Winter 2018) IFT 6085: Theoretical principles for deep learning](http://mitliagkas.github.io/ift6085-dl-theory-class/)
* http://principlesofdeeplearning.com/
* https://cbmm.mit.edu/education/courses
* [DALI 2018 - Data, Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
* [On Theory@http://www.deeplearningpatterns.com ](http://www.deeplearningpatterns.com/doku.php?id=theory)
* https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/85815724
* [UVA DEEP LEARNING COURSE](https://uvadlc.github.io/)
* [Understanding Neural Networks by embedding hidden representations](https://rakeshchada.github.io/Neural-Embedding-Animation.html)
* [Tractable Deep Learning](https://www.cs.washington.edu/research/tractable-deep-learning)
* [Theories of Deep Learning (STATS 385)](https://stats385.github.io/)
* [Topics Course on Deep Learning for Spring 2016 by Joan Bruna, UC Berkeley, Statistics Department](https://github.com/joanbruna/stat212b)
* [Mathematical aspects of Deep Learning](http://elmos.scripts.mit.edu/mathofdeeplearning/)
* [MATH 6380p. Advanced Topics in Deep Learning Fall 2018](https://deeplearning-math.github.io/)
* [CoMS E6998 003: Advanced Topics in Deep Learning](https://www.advancedtopicsindeeplearning.com/)
* [Deep Learning Theory: Approximation, Optimization, Generalization](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html)
* [Theory of Deep Learning, ICML'2018](https://sites.google.com/site/deeplearningtheory/)
* [DALI 2018, Data Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
* [MATHEMATICS OF DEEP LEARNING, NYU, Spring 2018](https://github.com/joanbruna/MathsDL-spring18)
* [Theory of Deep Learning, project in researchgate](https://www.researchgate.net/project/Theory-of-Deep-Learning)
* [THE THEORY OF DEEP LEARNING - PART I](https://physicsml.github.io/blog/DL-theory.html)
* [Magic paper](http://cognitivemedium.com/magic_paper/index.html)
* [Principled Approaches to Deep Learning](https://www.padl.ws/)
* [A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/pdf/1811.03962.pdf)
* [Advancing AI through cognitive science](https://github.com/brendenlake/AAI-site)
* [Deep Learning and the Demand for Interpretability](http://stillbreeze.github.io/Deep-Learning-and-the-Demand-For-Interpretability/)
* [Integrated and detailed image understanding](https://www.robots.ox.ac.uk/~vedaldi//research/idiu/idiu.html)
* [NeuroIP 2018 workshop on Deep Learning Theory](http://nips2018dltheory.rice.edu/)
* http://networkinterpretability.org/
* https://interpretablevision.github.io/
* https://www.msra.cn/zh-cn/news/people-stories/wei-chen
* https://www.microsoft.com/en-us/research/people/tyliu/
* https://zhuanlan.zhihu.com/p/22353056
* http://qszhang.com/index.php/team/
* https://www.researchgate.net/profile/Hatef_Monajemi
* [Symposium Artificial Intelligence for Science, Industry and Society](https://indico.cern.ch/event/781223/)
* https://arxiv.org/abs/1909.13458

### Deep Learning Reading Group

[yanjun](http://www.cs.virginia.edu//papers.htm) organized a wonderful reading group on deep learning.

- https://a2i2.deakin.edu.au/
- https://qdata.github.io/deep2Read/
- https://dlta-reading.github.io/
* http://www.mlnl.cs.ucl.ac.uk/readingroup.html
* https://labrosa.ee.columbia.edu/cuneuralnet/
* http://www.ub.edu/cvub/reading-group/
* https://team.inria.fr/perception/deeplearning/
* https://scholar.princeton.edu/csmlreading
* https://junjuew.github.io/elijah-reading-group/
* http://www.sribd.cn/DL/schedule.html
* http://lear.inrialpes.fr/people/gaidon/lear_xrce_deep_learning_01.html
* https://simons.berkeley.edu/events/reading-group-deep-learning
* https://csml.princeton.edu/readinggroup
* http://www.bicv.org/deep-learning/
* https://www.cs.ubc.ca/labs/lci/mlrg/
* https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
* https://project.inria.fr/deeplearning/
* https://hustcv.github.io/reading-list.html

## Interpretability of Neural Networks

[ Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks.](http://academic.hep.com.cn/fitee/CN/10.1631/FITEE.1700808#1)
At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations.
We believe that high model interpretability may help people break several bottlenecks of deep learning,
e.g., learning from a few annotations, learning via human–computer communications at the semantic level,
and semantically debugging network representations.
We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations,
methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model interpretability.
Finally, we discuss prospective trends in explainable artificial intelligence.

- https://www.transai.org/
- [GAMES Webinar 2019 – 93期(深度学习可解释性专题课程) ](http://games-cn.org/games-webinar-20190509-93/)
- [GAMES Webinar 2019 – 94期(深度学习可解释性专题课程) | 刘日升（大连理工大学），张拳石（上海交通大学）](http://games-cn.org/games-webinar-20190516-94/)
- http://qszhang.com/index.php/publications/
- [Explaining Neural Networks Semantically and Quantitatively](https://arxiv.org/abs/1812.07169)
- https://www.jiqizhixin.com/articles/0211
- https://www.jiqizhixin.com/articles/030205
- https://mp.weixin.qq.com/s/xY7Cpe6idbOTJuyD3vwD3w
- http://academic.hep.com.cn/fitee/CN/10.1631/FITEE.1700808#1
- https://arxiv.org/pdf/1905.11833.pdf
- http://www.cs.sjtu.edu.cn/~leng-jw/
- https://lemondan.github.io
- http://ise.sysu.edu.cn/teacher/teacher02/1136886.htm
- http://www.cs.cmu.edu/~zhitingh/data/hu18texar.pdf
- https://datasciencephd.eu/DSSS19/slides/GiannottiPedreschi-ExplainableAI.pdf
- http://www.cs.cmu.edu/~zhitingh/
- https://graphreason.github.io/
* https://beenkim.github.io/
* https://www.math.ucla.edu/~montufar/
* [Explainable AI: Interpreting, Explaining and Visualizing Deep Learning](https://link.springer.com/book/10.1007/978-3-030-28954-6)
* http://www.prcv2019.com/en/index.html
* http://gr.xjtu.edu.cn/web/jiansun
* http://www.shixialiu.com/
* http://irc.cs.sdu.edu.cn/
* https://www.seas.upenn.edu/~minchenl/
* https://cs.nyu.edu/~yixinhu/
* http://www.cs.utexas.edu/~huangqx/

Not all one can understand the relative theory or quantum theory.


## Physics and Deep Learning

[Neuronal networks have enjoyed a resurgence both in the worlds of neuroscience, where they yield mathematical frameworks for thinking about complex neural datasets, and in machine learning, where they achieve state of the art results on a variety of tasks, including machine vision, speech recognition, and language translation.   Despite their empirical success, a mathematical theory of how deep neural circuits, with many layers of cascaded nonlinearities, learn and compute remains elusive.  We will discuss three recent vignettes in which ideas from statistical physics can shed light on this issue.  In particular, we show how dynamical criticality can help in neural learning, how the non-intuitive geometry of high dimensional error landscapes can be exploited to speed up learning, and how modern ideas from non-equilibrium statistical physics, like the Jarzynski equality, can be extended to yield powerful algorithms for modeling complex probability distributions.  Time permitting, we will also discuss the relationship between neural network learning dynamics and the developmental time course of semantic concepts in infants.](https://physics.berkeley.edu/news-events/events/20151005/the-statistical-physics-of-deep-learning-on-the-beneficial-roles-of)

[In recent years, artificial intelligence has made remarkable advancements, impacting many industrial sectors dependent on complex decision-making and optimization. Physics-leaning disciplines also face hard inference problems in complex systems: climate prediction, density matrix estimation for many-body quantum systems, material phase detection, protein-fold quality prediction, parametrization of effective models of high-dimensional neural activity, energy landscapes of transcription factor-binding, etc. Methods using artificial intelligence have in fact already advanced progress on such problems. So, the question is not whether, but how AI serves as a powerful tool for data analysis in academic research, and physics-leaning disciplines in particular.](http://www.physics.mcgill.ca/ai2019/)

<img src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2017/09/InfoBottleneck_2880x1620.jpg" width="80%"/>

* https://zhuanlan.zhihu.com/p/94249675
* [Physics Informed Neural Networks](https://github.com/maziarraissi/PINNs)
* [Physics Based Vision meets Deep Learning (PBDL)](https://pbdl2019.github.io/)
* [Physics-Based Deep Learning](https://github.com/thunil/Physics-Based-Deep-Learning)
* [Physics Meets ML](https://www.microsoft.com/en-us/research/event/physics-ml-workshop/)
* [Physics in Machine Learning Workshop](https://bids.berkeley.edu/events/physics-machine-learning-workshop)
* [Physics in Machine Learning Workshop](https://www.ml4science.org/astrophysics-in-machine-learning-workshop)
* [Physics Informed Machine Learning Workshop](http://www.databookuw.com/page-5/)
* [physics forests](http://apagom.com/physicsforests/)
* [Applied Machine Learning Days](https://www.appliedmldays.org/)
* [Machine Learning in Physics School/Workshop](http://phys.cts.nthu.edu.tw/actnews/content.php?Sn=468)
* [DEEP LEARNING FOR MULTIMESSENGER ASTROPHYSICS: REAL-TIME DISCOVERY AT SCALE](http://www.ncsa.illinois.edu/Conferences/DeepLearningLSST/)
* [Workshop on Science of Data Science | (smr 3283)](http://indico.ictp.it/event/8722/)
* [Physics & AI Workshop](http://www.physics.mcgill.ca/ai2019/)
* https://www.icts.res.in/discussion-meeting/spmml2020
* https://physicsml.github.io/pages/papers.html
* http://super-ms.mit.edu/physics-ai.html
* https://dl4physicalsciences.github.io/
* https://maziarraissi.github.io/PINNs/
* http://deeplearnphysics.org/
* [Machine Learning for Physics and the Physics of Learning](https://www.ipam.ucla.edu/programs/long-programs/machine-learning-for-physics-and-the-physics-of-learning/)
* https://www.ias.edu/events/deep-learning-physics
* [A Differentiable Physics Engine for Deep Learning](http://phys.csail.mit.edu/papers/1.pdf)
* https://gogul.dev/software/deep-learning-meets-physics
* https://github.com/2prime/ODE-DL/blob/master/DL_Phy.md
* https://physics-ai.com/
* http://physics.usyd.edu.au/quantum/Coogee2015/Presentations/Svore.pdf
* [Brains, Minds and Machines Summer Course](https://ocw.mit.edu/resources/res-9-003-brains-minds-and-machines-summer-course-summer-2015/index.htm)
* [Physics-Based Deep Learning for Fluid Flow](http://phys2018.csail.mit.edu/papers/29.pdf)
* [deep medcine](http://amos3.aapm.org/abstracts/pdf/127-36916-419554-130797.pdf)
* http://www.dam.brown.edu/people/mraissi/publications/
* [Deep Learning in High Energy Physics](https://dlonsc.github.io/ISC2019/7_Keynote_DL_HEP_SofiaVallecorsa.pdf)
* [MATH + X SYMPOSIUM ON INVERSE PROBLEMS AND DEEP LEARNING IN SPACE EXPLORATION](https://earthscience.rice.edu/mathx2019/)
* [Machine Learning for Physics](https://machine-learning-for-physicists.org/)
* https://sites.google.com/view/icml2019phys4dl/schedule
* [Master-Seminar - Deep Learning in Physics (IN2107, IN0014)](https://www.in.tum.de/cg/teaching/winter-term-1819/deep-learning-in-physics/)
* https://www.ml4science.org/agenda-physics-in-ml
* [Theoretical Physics for Deep Learning](https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3531)
* http://www.physics.rutgers.edu/gso/SSPAR/
* https://community.singularitynet.io/c/education/course-brains-minds-machines
* [ARTIFICIAL INTELLIGENCE AND PHYSICS](https://physai.sciencesconf.org/)
* http://inspirehep.net/record/1680302/references
* https://www.pnnl.gov/computing/philms/Announcements.stm
* https://tacocohen.wordpress.com/
* https://cnls.lanl.gov/External/workshops.php
* https://www.researchgate.net/profile/Jinlong_Wu3
* http://djstrouse.com/
* https://www.researchgate.net/scientific-contributions/2135376837_Maurice_Weiler
* [Spontaneous Symmetry Breaking in Neural Networks](https://arxiv.org/abs/1710.06096)
* [2017 Machine Learning for Physicists, by Florian Marquardt](http://www.thp2.nat.uni-erlangen.de/index.php/2017_Machine_Learning_for_Physicists,_by_Florian_Marquardt)


### Statistical Mechanics and Deep Learning

[The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to shed conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.](https://www.annualreviews.org/doi/pdf/10.1146/annurev-conmatphys-031119-050745)

* [statistical mechanics // machine learning](http://smml.io/)
* [A Theoretical Connection Between Statistical Physics and Reinforcement Learning](https://arxiv.org/abs/1906.10228)
* [The thermodynamics of learning](https://phys.org/news/2017-02-thermodynamics.html)
* [WHY DOES DEEP LEARNING WORK?](https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/)
* [WHY DEEP LEARNING WORKS II: THE RENORMALIZATION GROUP](https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/)
*  https://github.com/CalculatedContent/ImplicitSelfRegularization
* [torbenkruegermath](https://sites.google.com/site/torbenkruegermath/home/graduate-seminar-random-matrices-spin-glasses-deep-learning)
* [TOWARDS A NEW THEORY OF LEARNING: STATISTICAL MECHANICS OF DEEP NEURAL NETWORKS](https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/)
* [Statistical Mechanics of Deep Learning](https://www.annualreviews.org/doi/pdf/10.1146/annurev-conmatphys-031119-050745)
* https://zhuanlan.zhihu.com/p/90096775

### Born Machine

- [Unsupervised Generative Modeling Using Matrix Product States](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031012#fulltext)
- https://github.com/congzlwag/UnsupGenModbyMPS
- https://congzlwag.github.io/UnsupGenModbyMPS/
- https://github.com/congzlwag/BornMachineTomo
- [From Baltzman machine to Born Machine](https://wangleiphy.github.io/talks/BornMachine.pdf)
- [Born Machines: A fresh approach to quantum machine learning](https://quantum.ustc.edu.cn/web/node/623)
- [Gradient based training of Quantum Circuit Born Machine (QCBM)](https://github.com/GiggleLiu/QuantumCircuitBornMachine)

### Quantum Machine learning

[Quantum Machine Learning: What Quantum Computing Means to Data Mining explains the most relevant concepts of machine learning, quantum mechanics, and quantum information theory, and contrasts classical learning algorithms to their quantum counterparts.](https://peterwittek.com/)

- [Combining quantum information and machine learning](https://www.quantummachinelearning.org/events.html)
- [machine learning for quantum technology/](https://www.mpl.mpg.de/divisions/marquardt-division/workshops/2019-machine-learning-for-quantum-technology/)
- https://wangleiphy.github.io/
- https://tacocohen.wordpress.com
- https://peterwittek.com/qml-in-2015.html
- https://github.com/krishnakumarsekar/awesome-quantum-machine-learning
- https://peterwittek.com/
* [Lecture Note on Deep Learning and Quantum Many-Body Computation](https://wangleiphy.github.io/lectures/DL.pdf)
* [Quantum Deep Learning and Renormalization](http://www.math.chalmers.se/~stig/project4.pdf)

____
* https://scholar.harvard.edu/madvani/home
* https://www.elen.ucl.ac.be/esann/index.php?pg=specsess#statistical
* https://krzakala.github.io/cargese.io/program.html
* [New Theory Cracks Open the Black Box of Deep Learning](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)
* [Unifying Physics and Deep Learning with TossingBot](https://ai.googleblog.com/2019/03/unifying-physics-and-deep-learning-with.html)


## Mathematics of Deep Learning

- [Meeting on Mathematics of Deep Learning](https://www.4tu.nl/ami/en/Agenda-Events/)
- [Probability in high dimensions](http://www.yanivplan.com/math-608d)
- [Learning Deep Learning](http://rt.dgyblog.com/ref/ref-learning-deep-learning.html)
- [Summer school on Deep Learning Theory by Weinan E](https://github.com/leiwu1990/course.math_theory_nn)
- [.520/6.860: Statistical Learning Theory and Applications, Fall 2018](http://www.mit.edu/~9.520/fall18/)
- [2018上海交通大学深度学习理论前沿研讨会 - 凌泽南的文章 - 知乎](https://zhuanlan.zhihu.com/p/40097048)
- [Theories of Deep Learning](https://www.researchgate.net/project/Theories-of-Deep-Learning)

[A mathematical theory of deep networks and of why they work as well as they do is now emerging. I will review some recent theoretical results on the approximation power of deep networks including conditions under which they can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. I will also discuss another puzzle around deep networks: what guarantees that they generalize and they do not overfit despite the number of weights being larger than the number of training data and despite the absence of explicit regularization in the optimization?](http://www.mit.edu/~9.520/fall17/Classes/deep_learning_theory.html)

Deep Neural Networks and Partial Differential Equations: Approximation Theory and
Structural Properties
Philipp Petersen, University of Oxford

- https://memento.epfl.ch/event/a-theoretical-analysis-of-machine-learning-and-par/
- http://at.yorku.ca/c/b/p/g/30.htm
- https://mat.univie.ac.at/~grohs/
- [Topics course Mathematics of Deep Learning, NYU, Spring 18](https://joanbruna.github.io/MathsDL-spring18/)
- https://skymind.ai/ebook/Skymind_The_Math_Behind_Neural_Networks.pdf
- https://github.com/markovmodel/deeptime
- https://omar-florez.github.io/scratch_mlp/
- https://joanbruna.github.io/MathsDL-spring19/
- https://github.com/isikdogan/deep_learning_tutorials
- https://www.brown.edu/research/projects/crunch/machine-learning-x-seminars
- [Deep Learning: Theory & Practice](http://anotherdatum.com/tce_2018.html)
- https://www.math.ias.edu/wtdl
- https://www.ml.tu-berlin.de/menue/mitglieder/klaus-robert_mueller/
- https://www-m15.ma.tum.de/Allgemeines/MathFounNN
- https://www.math.purdue.edu/~buzzard/MA598-Spring2019/index.shtml
- http://mathematics-in-europe.eu/?p=801
- [Discrete Mathematics of Neural Networks: Selected Topics](https://epubs.siam.org/doi/book/10.1137/1.9780898718539?mobileUi=0)
- https://cims.nyu.edu/~bruna/
- https://www.math.ias.edu/wtdl
- https://www.pims.math.ca/scientific-event/190722-pcssdlcm
- [Deep Learning for Image Analysis EMBL COURSE](https://www.embl.de/training/events/2020/MAC20-01/)
- [MATH 6380o. Deep Learning: Towards Deeper Understanding, Spring 2018](https://deeplearning-math.github.io/2018spring.html)
- [Mathematics of Deep Learning, Courant Insititute, Spring 19](https://github.com/joanbruna/MathsDL-spring19)
- http://voigtlaender.xyz/
- http://www.mit.edu/~9.520/fall19/
- [The Mathematics of Deep Learning and Data Science - Programme](https://gateway.newton.ac.uk/event/ofbw46/programme)
+ [Home of Math + Machine Learning + X](https://www.brown.edu/research/projects/crunch/)
+ [Mathematical and Computational Aspects of Machine Learning](http://crm.sns.it/event/451/)
+ [Mathematical Theory for Deep Neural Networks](https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks)
+ [Theory of Deep Learning](https://www.researchgate.net/project/Theory-of-Deep-Learning)
+ [DALI 2018 - Data, Learning and Inference](http://dalimeeting.org/dali2018/workshopTheoryDL.html)
+ [BMS Summer School 2019: Mathematics of Deep Learning](https://www.math-berlin.de/academics/summer-schools/2019)
+ [SIAM Conference on Mathematics of Data Science (MDS20)](https://www.siam.org/conferences/cm/conference/mds20)

* [BRIDGING GAME THEORY AND DEEP LEARNING](https://sgo-workshop.github.io/)


### Numerical Analysis for Deep Learning

Dynamics of deep learning is to  consider deep learning as a dynamic system. For example, the forward feedback network is expressed in the recurrent form:
$$x^{t+1} = f_t(x^{t}),t\in [0,1,\cdots, T]$$
where $f_t$ is some nonlinear function and $t$ is discrete.

However, it is not easy to select a proper nonlinear function $f_t \,\,\forall t\in[0,1,\cdots, T]$ and the number $T$.
In another word, there are no unified scientific principle or  guide to design the structure of deep neural network models.  

Many recursive formula share the same `feedback` forms or hidden structure, where the next input is the output of previous or historical record or generated points.


#### ResNets

`Deep Residual Networks` won the 1st places in: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
It inspired more efficient forward  convolutional networks.

They take a standard feed-forward ConvNet and add skip connections that bypass (or shortcut) a few convolution layers at a time. Each bypass gives rise to a residual block in which the convolution layers predict a residual that is added to the block’s input tensor.

<img src="https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/resnets_1.png" width="40%"/>

- https://github.com/KaimingHe/deep-residual-networks
- http://torch.ch/blog/2016/02/04/resnets.html
- https://zh.gluon.ai/chapter_convolutional-neural-networks/resnet.html
- https://www.jiqizhixin.com/articles/042201
- http://www.smartchair.org/hp/MSML2020/Paper/
- https://github.com/liuzhuang13/DenseNet
- https://arxiv.org/abs/1810.11741
- [Depth with nonlinearity creates no bad local minima in ResNets](https://www.sciencedirect.com/science/article/pii/S0893608019301820?via%3Dihub)

#### Differential Equations Motivated Deep Learning Methods

This section is on insight from numerical analysis to inspire more effective deep learning architecture.

[Many effective networks can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures.](https://web.stanford.edu/~yplu/proj/lm/)

 [We show that residual neural networks can be interpreted as discretizations of a nonlinear time-dependent ordinary differential equation that depends on unknown parameters, i.e., the network weights. We show how this insight has been used, e.g., to study the `stability of neural networks, design new architectures, or use established methods from optimal control methods for training ResNets`. Finally, we discuss open questions and opportunities for mathematical advances in this area.](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)


- https://elsc.huji.ac.il/all-publications/1050
- [NEURAL NETWORKS AS ORDINARY DIFFERENTIAL EQUATIONS](https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/)
- [Dynamical aspects of Deep Learning](https://zhenyu-liao.github.io/pdf/pre/GDD_iCODE.pdf)
- [Dynamical Systems and Deep Learning](http://www.doc.ic.ac.uk/~ae/teaching.html#complex)
- https://zhuanlan.zhihu.com/p/71747175
- https://web.stanford.edu/~yplu/
- https://web.stanford.edu/~yplu/project.html
- https://github.com/2prime/ODE-DL/
- [Deep Neural Networks Motivated by Partial Differential Equations](https://arxiv.org/pdf/1804.04272.pdf)
* https://www.researchgate.net/scientific-contributions/2107227289_Eldad_Haber


Residual networks as discretizations of dynamic systems:
$$
Y_1 = Y_0 +h \sigma(K_0 Y_0 + b_0)\\
\vdots  \\
Y_N = Y_{N-1} +h \sigma(K_{N-1} Y_{N-1} + b_{N-1})
$$

This is nothing but a forward Euler discretization of the `Ordinary Differential Equation (ODE)`:
$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

The goal is to plan a path (via $K$ and $b$) such that the initial data can be linearly separated.

<img src="http://www.mathcs.emory.edu/~lruthot/img/DeepLearning.png" width="80%" />

Another idea is to ensure stability by design / constraints on $\sigma$ and $K(t), b(t)$.

ResNet with antisymmetric transformation matrix:
$$\partial Y(t)=\sigma([K(t)-K(t)^T] Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

Hamiltonian-like ResNet
$$\frac{\mathrm d}{\mathrm d t}(Y(t), Z(t))^T=\sigma[(K(t)Z(t), -K(t)^T Y(t))^T + b(t)], t\in[0, T].$$

`Parabolic Residual Neural Networks`

$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

`Hyperbolic Residual Neural Networks`

$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$

`Hamiltonian CNN`

$$\partial Y(t)=\sigma(K(t) Y(t) + b(t)), Y(0)=Y_0, t\in[0, T].$$


- http://www.mathcs.emory.edu/~lruthot/talks/
- [CS 584 / MATH 789R - Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/math789r-sp20.html)
- [Numerical methods for deep learning](https://github.com/IPAIopen/NumDL-CourseNotes)
- [Short Course on Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)
- [CS 584 / MATH 789R - Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/math789r-sp20.html)
- [MA 721: Topics in Numerical Analysis: Deep Learning](http://www.ms.uky.edu/~qye/MA721/ma721F17.html)
- [Numerical methods for deep learning](https://github.com/IPAIopen/NumDL-CourseNotes)
- [Short Course on Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/index.html)
- [Deep Neural Networks Motivated By Ordinary Differential Equations](http://www.mathcs.emory.edu/~lruthot/talks/2019-LR-IPAM-ODE-handout.pdf)
- [Continuous Models: Numerical Methods for Deep Learning](http://www.mathcs.emory.edu/~lruthot/courses/NumDL/3-NumDNNshort-ContinuousModels.pdf)
- [Fully Hyperbolic Convolutional Neural Networks](https://arxiv.org/abs/1905.10484)

<img src="https://pic4.zhimg.com/80/v2-542db02f15d327ccc7558df7a8e6e137_hd.jpg" width="60%"/>

`Numerical differential equation inspired networks`:
$$Y_{t+1} = (1-k_t)Y_{t-1} + k_t Y_t + h \sigma(K_{t} Y_{t} + b_{t})\tag{Linear multi-step structure}.$$


- [Bridging Deep Architects and Numerical Differential Equations](https://web.stanford.edu/~yplu/proj/lm/)
- [BRIDGING DEEP NEURAL NETWORKS AND DIFFERENTIAL EQUATIONS FOR IMAGE ANALYSIS AND BEYOND](http://helper.ipam.ucla.edu/publications/glws3/glws3_15460.pdf)
- [Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations](https://arxiv.org/abs/1710.10121)
- http://bicmr.pku.edu.cn/~dongbin/
- https://arxiv.org/pdf/1906.02762.pdf  
- [Neural ODE Paper List](https://zhuanlan.zhihu.com/p/87999707)


* [A Multiscale and Multidepth Convolutional Neural Network for Remote Sensing Imagery Pan-Sharpening](https://ieeexplore.ieee.org/document/8281501)
* https://arxiv.org/abs/1808.02376
* [Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images](https://www.nature.com/articles/s41598-018-22871-z)

`MgNet`

[As the solution space is often the dual of the data space in PDEs, the
analogous concept of feature space and data space (which are dual to each other) is introduced
in CNN. With such connections and new concept in the unified model, the function of various
convolution operations and pooling used in CNN can be better understood.](https://arxiv.org/pdf/1901.10415.pdf)

- [MgNet: A Unified Framework of Multigrid and Convolutional Neural Network](https://arxiv.org/pdf/1901.10415.pdf)
- http://www.multigrid.org/img2019/img2019/Index/shortcourse.html
- https://deepai.org/machine-learning/researcher/jinchao-xu


_____


- [MA 721: Topics in Numerical Analysis: Deep Learning](http://www.ms.uky.edu/~qye/MA721/ma721F17.html)
- http://www.mathcs.emory.edu/~lruthot/teaching.html
- https://www.math.ucla.edu/applied/cam
- http://www.mathcs.emory.edu/~lruthot/
- [Automatic Differentiation of Parallelised Convolutional Neural Networks - Lessons from Adjoint PDE Solvers](https://autodiff-workshop.github.io/slides/Hueckelheim_nips_autodiff_CNN_PDE.pdf)
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs.](https://www.math.tu-berlin.de/fachgebiete_ag_modnumdiff/angewandtefunktionalanalysis/v_menue/mitarbeiter/kutyniok/v_menue/kutyniok_publications/)
- https://raoyongming.github.io/  



### Control Theory and Deep Learning

[It arose out of control theory literature when people were trying to identify highly complex and nonlinear dynamical systems. Neural networks – artificial neural networks – were first used in a supervised learning scenario in control theory. Hornik, if I remember correctly, was the first to find that neural networks were universal approximators.](http://scriptedonachip.com/ml-control)

> Supervised Deep Learning Problem
Given training data, $Y_0$, and labels, $C$, find network parameters $\theta$ and
classification weights $W, \mu$ such that the DNN predicts the data-label
relationship (and generalizes to new data), i.e., solve
$$\operatorname{minimize}_{ \theta,W,\mu} loss[g(W, \mu), C] + regularizer[\theta,W,\mu]$$

This can rewrite in a compact form
$$\operatorname{minimize}_{ \theta,W,\mu} loss[g(W(T)Y(T)+\mu), C] + regularizer[\theta,W,\mu]\\
\text{subject to  }\partial_t Y(t) = f (Y(t), \theta(t)), Y(0) = Y_0.$$

- [Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective](https://arxiv.org/abs/1908.10920)
- [An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks](http://proceedings.mlr.press/v80/li18b/li18b.pdf)
- [Dynamic System and Optimal Control Perspective of Deep Learning](https://web.stanford.edu/~yplu/DynamicOCNN.pdf)
- [A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1751636)
- [Deep learning as optimal control problems: models and numerical methods](https://arxiv.org/pdf/1904.05657.pdf)
- [A Mean-Field Optimal Control Formulation of Deep Learning](https://deepai.org/publication/a-mean-field-optimal-control-formulation-of-deep-learning)
- [Control Theory and Machine Learning](http://scriptedonachip.com/ml-control)
- [Advancing Systems and Control Research in the Era of ML and AI](https://faculty.sites.uci.edu/khargonekar/files/2018/04/Control_ML_AI_Final.pdf)
- http://marcogallieri.micso.it/Home.html
- [Deep Learning meets Control Theory: Research at NNAISENSE and Polimi](http://www.eventideib.polimi.it/events/deep-learning-meets-control-theory-research-at-nnaisense-and-polimi/)
- [Machine Learning-based Control](https://github.com/lakehanne/awesome-neurocontrol)
- [CAREER: A Flexible Optimal Control Framework for Efficient Training of Deep Neural Networks](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1751636)
- https://www.zhihu.com/question/315809187/answer/623687046


### Neural Ordinary Differential Equations

`Neural ODE`

- [Neural Ordinary Differential Equations](http://www.cs.toronto.edu/~rtqichen/pdfs/neural_ode_slides.pdf)

<img src="https://rkevingibson.github.io/img/ode_networks_1.png" width="80%" />

+ [NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data](https://www.arxiv-vanity.com/papers/1908.03190/)
+ [Neural Ordinary Differential Equations and Adversarial Attacks](https://rajatvd.github.io/Neural-ODE-Adversarial/)
+ [Neural Dynamics and Computation Lab](http://ganguli-gang.stanford.edu/)
+ [NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data](https://arxiv.org/abs/1908.03190)
+ https://math.ethz.ch/sam/research/reports.html?year=2019


## Dynamics and Deep Learning

- http://roseyu.com/
- [A Proposal on Machine Learning via Dynamical Systems](https://link.springer.com/article/10.1007/s40304-017-0103-z)
- http://www.scholarpedia.org/article/Attractor_network
- [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf)
- [An Attractor-Based Complexity Measurement for Boolean Recurrent Neural Networks](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3984152/)
- [Deep learning for universal linear embeddings of nonlinear dynamics](https://doaj.org/article/9d9172e9bf324cc6ac6d48ff8e234a85)
- [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](http://ganguli-gang.stanford.edu/pdf/DynamLearn.pdf)
- [Continuous attractors of higher-order recurrent neural networks with infinite neurons](https://www.sciencedirect.com/science/article/pii/S0925231213009338)
- https://www.researchgate.net/profile/Jiali_Yu3
- [Markov Transitions between Attractor States in a Recurrent Neural Network](https://cbmm.mit.edu/sites/default/files/publications/aaai-abstract%20%281%29.pdf)
- [A Survey on Machine Learning Applied to Dynamic Physical Systems](https://sagarverma.github.io/others/lit_rev_physics.pdf)
- https://deepdrive.berkeley.edu/project/dynamical-view-machine-learning-systems

### Deep Unrolling

Several recent studies build deep structures by unrolling a particular optimization model that involves task information, i.e., `learning to optime`.

Like gradient boost decision tree, we can optimize a cost function with a machine learning algorithms to fit the gradients- that is so-called gradient boost machine.
In another hand, it is expected that machine learning could learn/approximate the ierative formula of any optimization algorithms.

- [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)

[The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.](https://arxiv.org/abs/1606.04474)


`learning-based iterative methods `

[Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems. Recently, learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems. Nevertheless, integrating learnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights. Moreover, there is a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of such methods is a little bit vague. This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), a generic and provable paradigm for nonconvex inverse problems. Our theoretical analysis reveals that FIMA allows us to generate globally convergent trajectories for learning-based iterative methods. Meanwhile, the devised scheduling policies on flexible modules should also be beneficial for classical numerical methods in the nonconvex scenario. Extensive experiments on real applications verify the superiority of FIMA.](http://dutmedia.org/FIMA/)

- https://github.com/Heyi007/FIMA
- [Proximal Alternating Direction Network: A Globally Converged Deep Unrolling Framework](https://arxiv.org/abs/1711.07653)
- [A Bridging Framework for Model Optimization and Deep Propagation](http://papers.nips.cc/paper/7685-a-bridging-framework-for-model-optimization-and-deep-propagation)
- [On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems](http://dutmedia.org/FIMA/)
- [Deep unrolling](https://zhuanlan.zhihu.com/p/44003318)
- http://dutmedia.org/
- http://dlutir.dlut.edu.cn/Scholar/Detail/6711
- https://dblp.uni-trier.de/pers/hd/l/Liu:Risheng
- https://github.com/dlut-dimt
- https://www.researchgate.net/project/optimization-numerical-computation-optimal-control
- https://ankita-shukla.github.io/
- [Neural-network-based iterative learning control of nonlinear systems](https://www.sciencedirect.com/science/article/abs/pii/S0019057819303908)



## Differential Equation and Deep Learning

This section is on how to use deep learning or more general machine learning to solve  differential equation numerically.

[We derive upper bounds on the complexity of ReLU neural networks approximating the solution maps of parametric partial differential equations. In particular, without any knowledge of its concrete shape, we use the inherent low-dimensionality of the solution manifold to obtain approximation rates which are significantly superior to those provided by classical approximation results. We use this low dimensionality to guarantee the existence of a reduced basis. Then, for a large variety of parametric partial differential equations, we construct neural networks that yield approximations of the parametric maps not suffering from a curse of dimension and essentially only depending on the size of the reduced basis.](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)

- https://aimath.org/workshops/upcoming/deeppde/
- https://github.com/IBM/pde-deep-learning
- https://arxiv.org/abs/1804.04272
- https://deepai.org/machine-learning/researcher/weinan-e
- https://deepxde.readthedocs.io/en/latest/
- https://github.com/IBM/pde-deep-learning
- https://github.com/ZichaoLong/PDE-Net
- https://github.com/amkatrutsa/DeepPDE
- https://github.com/maziarraissi/DeepHPMs
- https://github.com/markovmodel/deeptime
- [The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems](https://link.springer.com/article/10.1007/s40304-018-0127-z)
- [Solving Nonlinear and High-Dimensional Partial Differential Equations via Deep Learning](http://utstat.toronto.edu/~ali/papers/PDEandDeepLearning.pdf)
- [Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations](https://arxiv.org/abs/1801.06637)
- [SPNets: Differentiable Fluid Dynamics for Deep Neural Networks](https://rse-lab.cs.washington.edu/papers/spnets2018.pdf)
- https://maziarraissi.github.io/DeepHPMs/
- [DGM: A deep learning algorithm for solving partial differential equations](https://www.sciencedirect.com/science/article/pii/S0021999118305527)
- [A Theoretical Analysis of Deep Neural Networks and Parametric PDEs](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Kutyniok/Papers/Parametric_PDEs_and_NNs_.pdf)
- [NeuralNetDiffEq.jl: A Neural Network solver for ODEs](https://julialang.org/blog/2017/10/gsoc-NeuralNetDiffEq)
- [PIMS CRG Summer School: Deep Learning for Computational Mathematics](https://www.pims.math.ca/scientific-event/190722-pcssdlcm)
- [Deep Approximation via Deep Learning](http://ins.sjtu.edu.cn:3300/conferences/7/talks/314)
* https://arxiv.org/abs/1806.07366
* https://mat.univie.ac.at/~grohs/
* https://rse-lab.cs.washington.edu/
* http://www.ajentzen.de/
* https://web.math.princeton.edu/~jiequnh/

$\mathcal H$ matrix and deep learning

[In this work we introduce a new multiscale artificial neural network based on the structure of H-matrices. This network generalizes the latter to the nonlinear case by introducing a local deep neural network at each spatial scale. Numerical results indicate that the network is able to efficiently approximate discrete nonlinear maps obtained from discretized nonlinear partial differential equations, such as those arising from nonlinear Schodinger equations and the KohnSham density functional theory.](https://web.stanford.edu/~lexing/mnnh.pdf)

* [A multiscale neural network based on hierarchical matrices](https://web.stanford.edu/~lexing/mnnh.pdf)
* [A multiscale neural network based on hierarchical nested bases](https://link.springer.com/article/10.1007%2Fs40687-019-0183-3)

[We aim to build a theoretical foundation for the analysis of deep neural networks to answer questions such as "What are the correct approximation spaces for deep neural networks?", "What is the advantage of deep versus shallow networks?", or "To which extent are deep neural networks able to detect low dimensional structures in high dimensional data?".](https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks)
- https://www.researchgate.net/profile/Gitta_Kutyniok
- https://www.researchgate.net/project/Mathematical-Theory-for-Deep-Neural-Networks
- https://www.academia-net.org/profil/prof-dr-gitta-kutyniok/1133890
- https://www.tu-berlin.de/index.php?id=168945
- https://www.math.tu-berlin.de/?108957
- [Deep Learning: An Introduction for Applied Mathematicians](https://arxiv.org/abs/1801.05894)

###  Stochastic Differential Equations and Deep Learning

- [Deep-Learning Based Numerical BSDE Method for Barrier Options](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3366314)
- [Machine learning approximation algorithms for high-dimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations](https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2017/2017-49.pdf)

## Approximation Theory for Deep Learning

Universal approximation theory show the expression power of deep neural network of some wide while shallow neural network.
The section will extend the approximation to the deep neural network.

[We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in $L^2(\mathbb R^d)$. In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy.](https://epubs.siam.org/doi/pdf/10.1137/18M118709X)

- [Approximation Analysis of Convolutional Neural Networks](https://cpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/d/11132/files/2019/07/paper_cnn_copy.pdf)
- [Deep vs. shallow networks : An approximation theory perspective](https://arxiv.org/abs/1608.03287)
- [Deep Neural Network Approximation Theory](https://arxiv.org/abs/1901.02220)
- [Provable approximation properties for deep neural networks](https://cpsc.yale.edu/sites/default/files/files/tr1513(1).pdf)
- [Optimal Approximation with Sparsely Connected Deep Neural Networks](https://epubs.siam.org/doi/pdf/10.1137/18M118709X)
- [Deep Learning: Approximation of Functions by Composition](http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf)
- [DGD Approximation Theory Workshop](https://www.math.tu-berlin.de/fileadmin/i26_fg-kutyniok/Petersen/DGD_Approximation_Theory.pdf)
- [Deep Neural Networks: Approximation Theory and Compositionality](http://www.mit.edu/~9.520/fall16/Classes/deep_approx.html)
- [DNN Bonn](http://voigtlaender.xyz/DNNBonnHandout.pdf)
- [From approximation theory to machine learning](http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf)
- [Collapse of Deep and Narrow Neural Nets](https://arxiv.org/abs/1808.04947)
- [Nonlinear Approximation and (Deep) ReLU Networks](https://www.math.tamu.edu/~foucart/publi/DDFHP.pdf)
- [Deep Approximation via Deep Learning](http://www.ipam.ucla.edu/abstract/?tid=15953&pcode=GLWS3)
- [Convolutional Neural Networks for Steady Flow Approximation](https://github.com/loliverhennigh/Steady-State-Flow-With-Neural-Nets)
- https://www.cityu.edu.hk/ma/people/profile/zhoudx.htm
- [Efficient approximation of high-dimensional functions with deep neural networks](https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2019/2019-64_fp.pdf)
- [Neural Jump SDEs (Jump Diffusions) and Neural PDEs](http://www.stochasticlifestyle.com/neural-jump-sdes-jump-diffusions-and-neural-pdes/)

### The F-Principle

[We aim to develop a theoretical framework on Fourier domain to analyze the Deep Neural Network (DNN) training process and understand the DNN generalization. We exemplified our theoretical results through DNNs fitting 1-d functions and the MNIST dataset.](https://www.researchgate.net/project/Deep-learning-in-Fourier-domain)

- [Deep learning in Fourier domain](https://www.researchgate.net/project/Deep-learning-in-Fourier-domain)
- [Deep Learning Theory: The F-Principle and An Optimization Framework](http://ins.sjtu.edu.cn:3300/conferences/7/talks/319)
- [Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks](https://arxiv.org/abs/1901.06523)
- [Nonlinear Collaborative Scheme for Deep Neural Networks](https://arxiv.org/abs/1811.01316)
- [The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies](https://arxiv.org/abs/1906.00425)
- [Frequency Principle in Deep Learning with General Loss Functions and Its Potential Application](https://arxiv.org/abs/1811.10146)
- [Theory of the Frequency Principle for General Deep Neural Networks](https://arxiv.org/pdf/1906.09235v1.pdf)
- [Explicitizing an Implicit Bias of the Frequency Principle in Two-layer Neural Networks](https://arxiv.org/pdf/1905.10264.pdf)
- https://www.researchgate.net/profile/Zhiqin_Xu

## Inverse Problem and Deep Learning

[There is a long history of algorithmic development for solving inverse problems arising in sensing and imaging systems and beyond. Examples include medical and computational imaging, compressive sensing, as well as community detection in networks. Until recently, most algorithms for solving inverse problems in the imaging and network sciences were based on static signal models derived from physics or intuition, such as wavelets or sparse representations.](https://deep-inverse.org/)

[Today, the best performing approaches for the aforementioned image reconstruction and sensing problems are based on deep learning, which learn various elements of the method including i) signal representations, ii) stepsizes and parameters of iterative algorithms, iii) regularizers, and iv) entire inverse functions. For example, it has recently been shown that solving a variety of inverse problems by transforming an iterative, physics-based algorithm into a deep network whose parameters can be learned from training data, offers faster convergence and/or a better quality solution. Moreover, even with very little or no learning, deep neural networks enable superior performance for classical linear inverse problems such as denoising and compressive sensing. Motivated by those success stories, researchers are redesigning traditional imaging and sensing systems.](https://deep-inverse.org/)

- [Sixteenth International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research](http://cpaior2019.uowm.gr/)
- [Neumann Networks for Inverse Problems in Imaging](https://arxiv.org/abs/1901.03707)
- https://github.com/mughanibu/Deep-Learning-for-Inverse-Problems
- [Accurate Image Super-Resolution Using Very Deep Convolutional Networks](https://cv.snu.ac.kr/research/VDSR/)
- [Deep Learning for Inverse Problems](https://arxiv.org/abs/1803.00092)
- [Solving inverse problems with deep networks](https://deep-inverse.org/)
- https://earthscience.rice.edu/mathx2019/
- [Deep Learning and Inverse Problem](https://www.dlip.org/)
- https://www.scec.org/publication/8768
- [deep inverse optimization](https://github.com/tankconcordia/deep_inv_opt)
- https://amds123.github.io/

## Random Matrix Theory and Deep Learning

Random matrix focus on the matrix, whose entities are sampled from  some specific probability distribution.
Weight matrices in deep nerual network are initialed in random.
However, the model is over-parametered and it is hard to verify the role of one individual parameter.

- https://project.inria.fr/paiss/
- https://zhenyu-liao.github.io/activities/
- [Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning](https://arxiv.org/abs/1810.01075)
- [Recent Advances in Random Matrix Theory for Modern Machine Learning](https://zhenyu-liao.github.io/pdf/pre/Matrix_talk_liao_handout.pdf)
- http://romaincouillet.hebfree.org/
- https://zhenyu-liao.github.io/
- https://dionisos.wp.imt.fr/
- [Features extraction using random matrix theory](https://ir.library.louisville.edu/cgi/viewcontent.cgi?article=2227&context=etd)
- [Nonlinear random matrix theory for deep learning](https://papers.nips.cc/paper/6857-nonlinear-random-matrix-theory-for-deep-learning.pdf)
- [A RANDOM MATRIX APPROACH TO NEURAL NETWORKS](https://arxiv.org/pdf/1702.05419.pdf)
- [A Random Matrix Approach to Echo-State Neural Networks](http://proceedings.mlr.press/v48/couillet16.pdf)
- [Harnessing neural networks: A random matrix approach](https://hal.archives-ouvertes.fr/hal-01962073)
- [Tensor Programs: A Swiss-Army Knife for Nonlinear Random Matrix Theory of Deep Learning and Beyond](https://www.csail.mit.edu/event/tensor-programs-swiss-army-knife-nonlinear-random-matrix-theory-deep-learning-and-beyond)
- [Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation](https://arxiv.org/abs/1902.04760)
- https://romaincouillet.hebfree.org/docs/conf/ELM_icassp.pdf
- https://romaincouillet.hebfree.org/docs/conf/NN_ICML.pdf
- http://www.vision.jhu.edu/tutorials/CVPR16-Tutorial-Math-Deep-Learning-Raja.pdf

## Deep learning and Optimal Transport

[Optimal transport (OT) provides a powerful and flexible way to compare probability measures, of all shapes: absolutely continuous, degenerate, or discrete. This includes of course point clouds, histograms of features, and more generally datasets, parametric densities or generative models. Originally proposed by Monge in the eighteenth century, this theory later led to Nobel Prizes for Koopmans and Kantorovich as well as Villani’s Fields Medal in 2010.](http://otml17.marcocuturi.net/)

- [Optimal Transport & Machine Learning](http://otml17.marcocuturi.net/)
- [Topics on Optimal Transport in Machine Learning and Shape Analysis (OT.ML.SA)](https://people.math.osu.edu/memolitechera.1/courses/cse-topics-2018/)
- https://www-obelix.irisa.fr/files/2017/01/postdoc-Obelix.pdf
- http://www.cis.jhu.edu/~rvidal/talks/learning/StructuredFactorizations.pdf
- http://cmsa.fas.harvard.edu/wp-content/uploads/2018/06/David_Gu_Harvard.pdf
- https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/
- https://www.louisbachelier.org/wp-content/uploads/2017/07/170620-ilb-presentation-gabriel-peyre.pdf
- http://people.csail.mit.edu/davidam/
- https://www.birs.ca/events/2020/5-day-workshops/20w5126
- https://github.com/hindupuravinash/nips2017
- [Selection dynamics for deep neural networks](https://arxiv.org/abs/1905.09076v1)
- https://people.math.osu.edu/memolitechera.1/index.html

## Geometric Analysis Approach to AI

[Why and how that deep learning works well on different tasks remains a mystery from a theoretical perspective. In this paper we draw a geometric picture of the deep learning system by finding its analogies with two existing geometric structures, the geometry of quantum computations and the geometry of the diffeomorphic template matching. In this framework, we give the geometric structures of different deep learning systems including convolutional neural networks, residual networks, recursive neural networks, recurrent neural networks and the equilibrium prapagation framework. We can also analysis the relationship between the geometrical structures and their performance of different networks in an algorithmic level so that the geometric framework may guide the design of the structures and algorithms of deep learning systems.](https://arxiv.org/pdf/1710.10784.pdf)

- [Machine Learning on Geometrical DataCSE291-C00 - Winter 2019](https://cse291-i.github.io/)
- [ABC Dataset A Big CAD Model Dataset For Geometric Deep Learning](https://deep-geometry.github.io/abc-dataset/)
- [Into the Wild: Machine Learning In Non-Euclidean Spaces](https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/)
- [How deep learning works — The geometry of deep learning](https://arxiv.org/pdf/1710.10784.pdf)
- http://cmsa.fas.harvard.edu/geometric-analysis-ai/
- http://inspirehep.net/record/1697651
- https://diglib.eg.org/handle/10.2312/2631996
- http://ubee.enseeiht.fr/skelneton/
- https://biomedicalimaging.org/2019/tutorials/
- [Machine Learning on Geometrical Data
CSE291-C00 - Winter 2019](https://cse291-i.github.io/)
- [Geometric View to Deep Learning](http://valser.org/article-269-1.html)
- [GEOMETRIC IDEAS IN MACHINE LEARNING: FROM DEEP LEARNING TO INCREMENTAL OPTIMIZATION](https://www.isi.edu/events/calendar/12459/)
- [Deep Learning Theory: Geometric Analysis of Capacity, Optimization, and Generalization for Improving Learning in Deep Neural Networks](https://cordis.europa.eu/project/rcn/214602/factsheet/en)
- [Workshop IV: Deep Geometric Learning of Big Data and Applications](http://www.ipam.ucla.edu/programs/workshops/workshop-iv-deep-geometric-learning-of-big-data-and-applications/)
- [Robustness and geometry of deep neural networks](https://gateway.newton.ac.uk/sites/default/files/asset/doc/1905/Alhussein_Fawzi.pdf)
- [A geometric view of optimal transportation and generative model](https://www.sciencedirect.com/science/article/pii/S0167839618301249)
- [Optimal Transport Theory the New Math for Deep Learning](https://mc.ai/optimal-transport-theory-the-new-math-for-deep-learning/)
- [GeoNet: Deep Geodesic Networks for Point Cloud Analysis](http://openaccess.thecvf.com/content_CVPR_2019/papers/He_GeoNet_Deep_Geodesic_Networks_for_Point_Cloud_Analysis_CVPR_2019_paper.pdf)
- http://www.stat.uchicago.edu/~lekheng/
- https://www.nsf.gov/awardsearch/showAward?AWD_ID=1418255
- https://nsf-tripods.org/institutes/
- https://users.math.msu.edu/users/wei/
- https://www.darpa.mil/program/hierarchical-identify-verify-exploit
- [The Loss Surface Of Deep Linear Networks Viewed
Through The Algebraic Geometry Lens](http://www.tianranchen.org/research/papers/deep-linear.pdf)

### Tropical Geometry of Deep Neural Networks

- [Tropical Geometry of Deep Neural Networks](https://arxiv.org/pdf/1805.07091.pdf)
- https://opendatagroup.github.io/data%20science/2019/04/11/tropical-geometry.html
- https://www.stat.uchicago.edu/~lekheng/
- https://mathsites.unibe.ch/siamag19/
- https://www.math.ubc.ca/~erobeva/seminar.html
- https://sites.google.com/view/maag2019/home
- https://sites.google.com/site/feliper84/

## Topology and Deep Learning

[We perform topological data analysis on the internal states of convolutional deep neural networks to develop an understanding of the computations that they perform. We apply this understanding to modify the computations so as to (a) speed up computations and (b) improve generalization from one data set of digits to another. One byproduct of the analysis is the production of a geometry on new sets of features on data sets of images, and use this observation to develop a methodology for constructing analogues of CNN's for many other geometries, including the graph structures constructed by topological data analysis.](https://arxiv.org/abs/1811.01122)

- [Topological Methods for Machine Learning](http://topology.cs.wisc.edu/)
- [A Topology Layer for Machine Learning](http://ai.stanford.edu/blog/topologylayer/)
- [Topological Approaches to Deep Learning](https://arxiv.org/abs/1811.01122)
- https://www.gaotingran.com/
- [Topology based deep learning for biomolecular data](https://users.math.msu.edu/users/wei/AIM.pdf)
- [RESEARCH ARTICLE TopologyNet: Topology based deep convolutional and multi-task neural networks for biomolecular property predictions](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005690)
- [Exposition and Interpretation of the Topology of Neural Networks](https://arxiv.org/abs/1810.03234)
- https://zhuanlan.zhihu.com/p/26515275
- [Applying Topological Persistence in Convolutional Neural Network for Music Audio Signals](https://arxiv.org/pdf/1608.07373.pdf)
- [Towards a topological–geometrical theory of group equivariant non-expansive operators for data analysis and machine learning](https://www.nature.com/articles/s42256-019-0087-3)

## Algebra and Deep Learning

Except the matrix and tensor decomposotion for accelerating the deep neural network, `Tensor network` is close to deep learning model.

- http://people.cs.uchicago.edu/~risi/
- https://ttic.uchicago.edu/~shubhendu/

### Tensor network

- [Linear Algebra and Learning from Data](http://math.mit.edu/~gs/learningfromdata/)
- [Accelerating deep neural networks with tensor decompositions](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning)
- [An Algebraic Perspective on Deep Learning](http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10605.pdf)
- [Tensor Networks in a Nutshell](https://arxiv.org/abs/1708.00006)
- [A library for easy and efficient manipulation of tensor networks.](https://github.com/google/tensornetwork)
- http://tensornetworktheory.org/
- https://www.perimeterinstitute.ca/research/research-initiatives/tensor-networks-initiative
- https://github.com/emstoudenmire/TNML
- http://itensor.org/
- http://users.cecs.anu.edu.au/~koniusz/
- https://deep-learning-tensorflow.readthedocs.io/en/latest/

### Group Equivariant Convolutional Networks

- https://github.com/tscohen/gconv_experiments
- http://dalimeeting.org/dali2019b/workshop-05-02.html
- https://erikbekkers.bitbucket.io/
- https://staff.fnwi.uva.nl/m.welling/
- https://www.ics.uci.edu/~welling/
- http://ibis.t.u-tokyo.ac.jp/suzuki/
- http://www.mit.edu/~kawaguch/
- https://www.4tu.nl/ami/en/Agenda-Events/

### Quaternion Neural Networks

[It looks like Deep (Convolutional) Neural Networks are really powerful. However, there are situations where they don’t deliver as expected. I assume that perhaps many are happy with pre-trained VGG, Resnet, YOLO, SqueezeNext, MobileNet, etc. models because they are “good enough”, even though they break quite easily on really realistic problems and require tons of training data. IMHO there are much smarter approaches out there, which are neglected/ignored. I don’t want to argue why they are ignored but I want to provide a list with other useful architectures.](https://www.simonwenkel.com/2019/07/15/Capsule-Networks-and-other-neural-architectures.html)

[Instead of staying with real numbers, we should have a look at complex numbers as well. Let’s remember the single reason why we use complex numbers ($C$) or quaternions ($\mathcal H$). The most important reason why we use complex numbers is not to solve $x^2=−1$. The reason why we use complex numbers for everything that involves waves etc. is that we are lazy or efficient ;). Who wants to waste time writing down and solving a bunch of trignometric identities. The same is true for quaternions in robotics. Speaking in terms of computer science, we are using a much more efficient data structure/representation. It seems like complex valued neural networks as well as quaternion, which are a different kind of complex numbers for the mathematical correct reader of this post, seem to outperform real valued neural networks while using less parameters. This makes sense because we are using a different data structure that itself helps to represent certain things in a much more useful way.](https://www.simonwenkel.com/2019/07/15/Capsule-Networks-and-other-neural-architectures.html)

- https://arxiv.org/abs/1903.08478
- [Introduction to Quaternion Neural Networks](https://www.simonwenkel.com/projects/introduction-to-quaternion-neural-networks.html)
- [Capsule Networks and other neural architectures that are less known](https://www.simonwenkel.com/2019/07/15/Capsule-Networks-and-other-neural-architectures.html)

## Probabilistic Theory and Deep Learning


[Probabilistic Deep Learning with Python teaches the increasingly popular probabilistic approach to deep learning that allows you to tune and refine your results more quickly and accurately without as much trial-and-error testing. Emphasizing practical techniques that use the Python-based Tensorflow Probability Framework, you’ll learn to build highly-performant deep learning applications that can reliably handle the noise and uncertainty of real-world data.](https://www.manning.com/books/probabilistic-deep-learning-with-python)

- [Probabilistic Framework for Deep Learning](https://ankitlab.co/projects/)
- [A Probabilistic Theory of Deep Learning](https://arxiv.org/abs/1504.00641)
- [A Probabilistic Framework for Deep Learning](https://papers.nips.cc/paper/6231-a-probabilistic-framework-for-deep-learning.pdf)
- [Lightweight Probabilistic Deep Networks](https://zpascal.net/cvpr2018/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.pdf)
- [Deep Probabilistic Programming](https://arxiv.org/abs/1701.03757)
- https://github.com/oxmlcs/ML_bazaar/wiki/Deep-Learning-and-Probabilistic-Inference
- https://eng.uber.com/pyro/
- [Probabilistic Deep Learning with Python](https://www.manning.com/books/probabilistic-deep-learning-with-python)
- https://livebook.manning.com/book/probabilistic-deep-learning/
- http://csml.stats.ox.ac.uk/
- https://fcai.fi/agile-probabilistic
- http://bayesiandeeplearning.org/2017/papers/59.pdf
- [GluonTS: Probabilistic Time Series Models in Python](https://arxiv.org/abs/1906.05264v2)
- [CS 731: Advanced methods in artificial intelligence, with biomedical applications (Fall 2009)](http://pages.cs.wisc.edu/~dpage/cs731/)
- [CS 838 (Spring 2004): Statistical Relational Learning](https://www.biostat.wisc.edu/~page/838.html)
- https://www.ida.liu.se/~ulfni53/lpp/bok/bok.pdf
- https://www.biostat.wisc.edu/bmi576/
- http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html

### Bayesian Deep Learning

[The abstract of Bayesian Deep learning](http://bayesiandeeplearning.org/) put that:

> While deep learning has been revolutionary for machine learning, most modern deep learning models cannot represent their uncertainty nor take advantage of the well studied tools of probability theory. This has started to change following recent developments of tools and techniques combining Bayesian approaches with deep learning. The intersection of the two fields has received great interest from the community over the past few years, with the introduction of new deep learning models that take advantage of Bayesian techniques, as well as Bayesian models that incorporate deep learning elements [1-11]. In fact, the use of Bayesian techniques in deep learning can be traced back to the 1990s’, in seminal works by Radford Neal [12], David MacKay [13], and Dayan et al. [14]. These gave us tools to reason about deep models’ confidence, and achieved state-of-the-art performance on many tasks. However earlier tools did not adapt when new needs arose (such as scalability to big data), and were consequently forgotten. Such ideas are now being revisited in light of new advances in the field, yielding many exciting new results
> Extending on last year’s workshop’s success, this workshop will again study the advantages and disadvantages of such ideas, and will be a platform to host the recent flourish of ideas using Bayesian approaches in deep learning and using deep learning tools in Bayesian modelling. The program includes a mix of invited talks, contributed talks, and contributed posters. It will be composed of five themes: deep generative models, variational inference using neural network recognition models, practical approximate inference techniques in Bayesian neural networks, applications of Bayesian neural networks, and information theory in deep learning. Future directions for the field will be debated in a panel discussion.
> This year’s main theme will focus on applications of Bayesian deep learning within machine learning and outside of it.

1. Kingma, DP and Welling, M, "Auto-encoding variational Bayes", 2013.
2. Rezende, D, Mohamed, S, and Wierstra, D, "Stochastic backpropagation and approximate inference in deep generative models", 2014.
3. Blundell, C, Cornebise, J, Kavukcuoglu, K, and Wierstra, D, "Weight uncertainty in neural network", 2015.
4. Hernandez-Lobato, JM and Adams, R, "Probabilistic backpropagation for scalable learning of Bayesian neural networks", 2015.
5. Gal, Y and Ghahramani, Z, "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", 2015.
6. Gal, Y and Ghahramani, G, "Bayesian convolutional neural networks with Bernoulli approximate variational inference", 2015.
7. Kingma, D, Salimans, T, and Welling, M. "Variational dropout and the local reparameterization trick", 2015.
8. Balan, AK, Rathod, V, Murphy, KP, and Welling, M, "Bayesian dark knowledge", 2015.
9. Louizos, C and Welling, M, “Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors”, 2016.
10. Lawrence, ND and Quinonero-Candela, J, “Local distance preservation in the GP-LVM through back constraints”, 2006.
11. Tran, D, Ranganath, R, and Blei, DM, “Variational Gaussian Process”, 2015.
12. Neal, R, "Bayesian Learning for Neural Networks", 1996.
13. MacKay, D, "A practical Bayesian framework for backpropagation networks", 1992.
14. Dayan, P, Hinton, G, Neal, R, and Zemel, S, "The Helmholtz machine", 1995.
15. Wilson, AG, Hu, Z, Salakhutdinov, R, and Xing, EP, “Deep Kernel Learning”, 2016.
16. Saatchi, Y and Wilson, AG, “Bayesian GAN”, 2017.
17. MacKay, D.J.C. “Bayesian Methods for Adaptive Models”, PhD thesis, 1992.

***

* [Towards Bayesian Deep Learning: A Framework and Some Existing Methods](https://arxiv.org/abs/1608.06884)
* http://www.wanghao.in/mis.html
* https://github.com/junlulocky/bayesian-deep-learning-notes
* https://github.com/robi56/awesome-bayesian-deep-learning
* https://alexgkendall.com/computer_vision/phd_thesis/
* http://bayesiandeeplearning.org/
* http://www.cs.ox.ac.uk/people/yarin.gal/website/blog.html
* http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/
* https://uvadlc.github.io/lectures/apr2019/lecture9-bayesiandeeplearning.pdf


## Statistics and Deep Learning

[A History of Deep Learning](https://www.import.io/post/history-of-deep-learning/)
>Mathematician Ivakhnenko and associates including Lapa arguably created the first working deep learning networks in 1965,
>applying what had been only theories and ideas up to that point.
>
>Ivakhnenko developed the Group Method of Data Handling (GMDH) –
>defined as a “family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets
>that features fully automatic structural and parametric optimization of models” –
>and applied it to neural networks.
>
>For that reason alone, many consider Ivakhnenko the father of modern deep learning.
>
>His learning algorithms used deep feedforward multilayer perceptrons using statistical methods at each layer to find the best features and forward them through the system.
>
>Using GMDH, Ivakhnenko was able to create an 8-layer deep network in 1971,
>and he successfully demonstrated the learning process in a computer identification system called Alpha.

- https://zhuanlan.zhihu.com/p/36519666
- http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/
- http://blog.shakirm.com/wp-content/uploads/2015/07/SVDL.pdf
- [On Statistical Thinking in Deep Learning: A Blog Post](http://bulletin.imstat.org/wp-content/uploads/ml-LONG_On_Statistical_Thinking_in_Deep_Learning.pdf)
- [Implementing Bayesian Inference with Neural Networks](http://ul.qucosa.de/api/qucosa%3A34703/attachment/ATT-0/)

## Information Theory and Deep Learning

[In short, Neural Networks extract from the data the most relevant part of the information that describes the statistical dependence between the features and the labels. In other words, the size of a Neural Networks specifies a data structure that we can compute and store, and the result of training the network is the best approximation of the statistical relationship between the features and the labels that can be represented by this data structure.](https://lizhongresearch.miraheze.org/wiki/Understanding_the_Power_of_Neural_Networks)

[In this talk, we formulate a new problem called the "universal feature selection" problem, where we need to select from the high dimensional data a low dimensional feature that can be used to solve, not one, but a family of inference problems. We solve this problem by developing a new information metric that can be used to quantify the semantics of data, and by using a geometric analysis approach. We then show that a number of concepts in information theory and statistics such as the HGR correlation and common information are closely connected to the universal feature selection problem. At the same time, a number of learning algorithms, PCA, Compressed Sensing, FM, deep neural networks, etc., can also be interpreted as implicitly or explicitly solving the same problem, with various forms of constraints.](https://lids.mit.edu/news-and-events/events/information-theoretic-interpretation-deep-neural-networks)

* http://ita.ucsd.edu/
* http://naftali-tishby.mystrikingly.com/
* http://lizhongzheng.mit.edu/
* [Information Theory of Deep Learning](https://adityashrm21.github.io/Information-Theory-In-Deep-Learning/)
* [Anatomize Deep Learning with Information Theory](https://lilianweng.github.io/lil-log/2017/09/28/anatomize-deep-learning-with-information-theory.html)
* [“Deep learning - Information theory & Maximum likelihood.”](https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/)
* [On the information bottleneck theory of deep learning](https://www.researchgate.net/publication/325022755_On_the_information_bottleneck_theory_of_deep_learning)
* [Deep Learning and the Information Bottleneck Principle](https://arxiv.org/pdf/1503.02406.pdf)
* [Information Theoretic Interpretation of Deep Neural Networks](https://lids.mit.edu/news-and-events/events/information-theoretic-interpretation-deep-neural-networks)
- http://pirsa.org/18040050
- https://lizhongresearch.miraheze.org/wiki/Main_Page
- https://lizhongzheng.mit.edu/
- https://lizhongresearch.m.miraheze.org/wiki/Main_Page
- https://www.leiphone.com/news/201703/qzBcOeDYFHtYwgEq.html
- http://nsfcbl.org/
- [Large Margin Deep Neural Networks: Theory and Algorithms](https://arxiv.org/abs/1506.05232v1)
- http://ai.stanford.edu/
- https://www.math.ias.edu/wtdl
- [DEEP 3D REPRESENTATION LEARNING](http://ai.ucsd.edu/~haosu/papers/thesis_finalversion.pdf)
- https://www.mis.mpg.de/ay/index.html
- https://www.math.ucdavis.edu/~strohmer/courses/180BigData/180BigData_info.html
- https://www.tbsi.edu.cn/index.php?s=/cms/181.html

## Brain Science and AI

[Artificial intelligence and brain science have had a swinging relationship of convergence and divergence. In the early days of pattern recognition, multi-layer neural networks based on the anatomy and physiology of the visual cortex played a key role, but subsequent sophistication of machine learning promoted methods that are little related to the brain. Recently, however, the remarkable success of deep neural networks in learning from big data has re-evoked the interests in brain-like artificial intelligence.](http://www.brain-ai.jp/project-outline/)

<img src="http://www.brain-ai.jp/wp-content/uploads/2017/01/brain.png" width="70%" />

- [Theoretical Neuroscience and Deep Learning Theory](http://videolectures.net/deeplearning2017_ganguli_deep_learning_theory/)
- [Bridging Neuroscience and Deep Machine Learning, by building theories that work in the Real World.](https://ankitlab.co/)
* [Center for Mind, Brain, Computation and Technology](https://neuroscience.stanford.edu/mbct/home)
* [Where neuroscience and artificial intelligence converge.](https://braininspired.co/about/)
* https://elsc.huji.ac.il/events/elsc-conference-10
* http://www.brain-ai.jp/organization/
* [Artificial Intelligence and brain](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5818638/)
* [Dissecting Artificial Intelligence to Better Understand the Human Brain](https://www.cogneurosociety.org/dissecting-artificial-intelligence-to-better-understand-the-human-brain/)
* [Deep Learning and the Brain](https://quest.mit.edu/projects/)
* [AI and Neuroscience: A virtuous circle](https://deepmind.com/blog/ai-and-neuroscience-virtuous-circle/)
* [Neuroscience-Inspired Artificial Intelligence](http://www.columbia.edu/cu/appliedneuroshp/Papers/out.pdf)
* [深度神经网络（DNN）是否模拟了人类大脑皮层结构？ - Harold Yue的回答 - 知乎](https://www.zhihu.com/question/59800121/answer/184888043)
* [Deep Learning: Branching into brains](https://elifesciences.org/articles/33066)

## Cognition Science and Deep Learning

Brain science is the physological theorey of cognitive science, which focus on the physical principle of brain function.
The core problem of cognition science is how to learn in my eyes.

[Artificial deep neural networks (DNNs) initially inspired by the brain enable computers to solve cognitive tasks at which humans excel.
In the absence of explanations for such cognitive phenomena, in turn cognitive scientists have started using DNNs as models to investigate biological cognition and its neural basis, creating heated debate.](https://www.sciencedirect.com/science/article/pii/S1364661319300348)

<img src="https://ars.els-cdn.com/content/image/1-s2.0-S1364661319300348-gr2.jpg" width="69%"/>

* https://www.mis.mpg.de/ay/
* [Josh Tenenbaum](http://web.mit.edu/cocosci/josh.html)
* [Deep Neural Networks as Scientific Models](https://www.sciencedirect.com/science/article/pii/S1364661319300348)
* https://wiki.opencog.org/w/Language_learning
* https://github.com/opencog/learn
* [Advancing AI through cognitive science - Spring 2019](https://brendenlake.github.io/AAI-site/)
* [NYU PSYCH-GA 3405.001 / DS-GA 3001.014 : Advancing AI through cognitive science](https://github.com/brendenlake/AAI-site)
* [PSYCH 209: Neural Network Models of Cognition: Principles and Applications](https://web.stanford.edu/class/psych209/)
* [Computational Learning and Memory Group](http://cbl.eng.cam.ac.uk/Public/Lengyel/News)
* [Computational cognitive Science Group @MIT](http://cocosci.mit.edu/)
* [Beyond deep learning](http://beyond-deep-nets.clps.brown.edu/)
* [Cognitive Computation Group @ U. Penn.](https://cogcomp.org/)
* [Computational cognitive modeling](https://brendenlake.github.io/CCM-site/)
* [Mechanisms of geometric cognition](http://hohol.pl/granty/geometry/)
* [Computational Cognitive Science Lab](http://cocosci.princeton.edu/research.php)
* [Deep Learning for Cognitive Computing, Theory](http://www.cs.jyu.fi/ai/vagan/DL4CC.html)
* [TOPIC: FROM ARISTOTLE TO WILLIAM JAMES TO DEEP LEARNING : EVERYTHING OLD IS NEW AGAIN.](https://iccs2019.github.io/#James-Anderson)
* https://www.dcsc.es/
* [Deep Neural Networks as Scientific Models](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930034-8)
* http://vca.ele.tue.nl/
* https://deepmind.com/research/publications/psychlab-psychology-laboratory-deep-reinforcement-learning-agents
* https://www.uni-potsdam.de/en/mlcog/index.html
* https://csai.nl/home/
* https://hadrienj.github.io/about/
* https://iccs2019.github.io/
* https://human-memory.net/
* https://sites.google.com/view/goergen
* https://engineering.purdue.edu/IE/people/ptProfile?resource_id=126302
* https://engineering.columbia.edu/faculty/christos-papadimitriou
* https://www.bio.purdue.edu/People/faculty_dm/directory.php?refID=1000000303
* https://people.csail.mit.edu/mirrokni/Welcome.html
* https://www.mindcogsci.net/

## The lottery ticket hypothesis

[The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training  by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019).](https://arxiv.org/pdf/1906.02768.pdf)
- https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks
- https://arxiv.org/pdf/1905.13405.pdf
- https://arxiv.org/abs/1903.01611
- https://arxiv.org/abs/1905.13405
- https://arxiv.org/abs/1906.02768
- https://arxiv.org/abs/1906.02773
- https://arxiv.org/abs/1909.13458
- https://zhuanlan.zhihu.com/p/84178021
- https://zhuanlan.zhihu.com/p/67782029
- https://openai.com/blog/deep-double-descent/

## AOGNet

[An AOGNet consists of a number of stages each of which is composed of a number of AOG building blocks. An AOG building block splits its input feature map into $N$ groups along feature channels and then treat it as a sentence of $N$ words. It then jointly realizes a phrase structure grammar and a dependency grammar in bottom-up parsing the “sentence” for better feature exploration and reuse. It provides a unified framework for the best practices developed in state-of-the-art DNNs.](https://arxiv.org/pdf/1711.05847.pdf)

`We first need to understand the underlying wisdom in designing better network architectures: It usually lies in finding network structures
which can support flexible and diverse information flows for exploring new features, reusing existing features in previous layers 
and back-propagating learning signals (e.g., gradients).`

- [AOGNets: Compositional Grammatical Architectures for Deep Learning](https://arxiv.org/pdf/1711.05847.pdf)
- http://www.stat.ucla.edu/~tfwu/
- http://www.stat.ucla.edu/~tfwu//project_posts/iRCNN/
- https://github.com/xilaili/AOGNet
- https://github.com/iVMCL/AOGNets
- http://www.stat.ucla.edu/~tfwu/project_posts/AOGNets/
- [New Framework Improves Performance of Deep Neural Networks](https://research.ece.ncsu.edu/ivmcl/2019/05/29/new-framework-improves-performance-of-deep-neural-networks/)
